{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rlTJ4iZ3XNlS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import kagglehub\n",
        "import pandas as pd\n",
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "import optuna\n",
        "import wandb\n",
        "from tqdm.notebook import tqdm\n",
        "import gc\n",
        "import random\n",
        "import time\n",
        "import html\n",
        "import re\n",
        "import wandb\n",
        "from IPython.display import display\n",
        "\n",
        "# PyTorch and Sklearn\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch import nn, optim\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Hugging Face\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n",
        "from datasets import Dataset as HFDataset\n",
        "\n",
        "# Model Compression\n",
        "from torch.nn.utils import prune\n",
        "from torch.quantization import quantize_dynamic\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cqQw4HZ6XSiS"
      },
      "outputs": [],
      "source": [
        "# --- Global Settings ---\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "d7OiGrD9XSlg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "set_seed(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JhqGXt0LXSoX"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/matan/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmoti-matan\u001b[0m (\u001b[33mmoti-matan-tel-aviv-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Login to W&B (you might be prompted for an API key)\n",
        "wandb.login(key=\"120e017ed2eaa1fa329d9b080c6b901366a51acb\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_tweet_text(text: str) -> str:\n",
        "    # Decode HTML entities\n",
        "    text = html.unescape(text)\n",
        "    # Replace URLs with <URL> token\n",
        "    text = re.sub(r'http\\S+|www\\S+', '<URL>', text)\n",
        "    # Replace mentions with <USER> token: none of the usernames are relevant for context and were probably anonimized.\n",
        "    text = re.sub(r'@\\w+', '<USER>', text)\n",
        "    # Optionally remove hashtag symbol but keep the word ( #covid -> covid )\n",
        "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
        "    # Normalize excessive repeated characters (e.g., sooooo → soo), also double or tripple spaces are a waste of tokens.\n",
        "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
        "    # Remove leading/trailing whitespace and normalize internal spaces\n",
        "    text = text.strip()\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JN1oo3fZXSqv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading dataset...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set size: 32925\n",
            "Validation set size: 8232\n",
            "Test set size: 3798\n"
          ]
        }
      ],
      "source": [
        "print(\"Downloading dataset...\")\n",
        "data_path = kagglehub.dataset_download(\"datatattle/covid-19-nlp-text-classification\")\n",
        "\n",
        "train_df_full = pd.read_csv(os.path.join(data_path, \"Corona_NLP_train.csv\"), encoding=\"latin1\")\n",
        "test_df = pd.read_csv(os.path.join(data_path, \"Corona_NLP_test.csv\"), encoding=\"latin1\")\n",
        "\n",
        "train_df_full['clean_text'] = train_df_full['OriginalTweet'].apply(clean_tweet_text)\n",
        "test_df['clean_text'] = test_df['OriginalTweet'].apply(clean_tweet_text)\n",
        "\n",
        "train_df, val_df = train_test_split(train_df_full, test_size=0.2, random_state=42, stratify=train_df_full['Sentiment'])\n",
        "\n",
        "print(f\"Train set size: {len(train_df)}\")\n",
        "print(f\"Validation set size: {len(val_df)}\")\n",
        "print(f\"Test set size: {len(test_df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3730     <USER> To everyone hoarding rice who until now...\n",
              "35121    If your going to eat <USER> they have compleme...\n",
              "9893     Watch this if you are one of those idiots who ...\n",
              "34429    We need to have a risk management system more ...\n",
              "29290    Markets plunge puts pension freedoms to the te...\n",
              "Name: clean_text, dtype: object"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df['clean_text'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8dFdodsNXStp"
      },
      "outputs": [],
      "source": [
        "class TweetsDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer):\n",
        "        sentiment_mapping = {'Extremely Negative': 0, 'Negative': 1, 'Neutral': 2, 'Positive': 3, 'Extremely Positive': 4}\n",
        "        self.texts = dataframe['clean_text'].tolist()\n",
        "        self.labels = dataframe['Sentiment'].map(sentiment_mapping).tolist()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = 512\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_len, return_tensors='pt')\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2Hm0ntONXflT"
      },
      "outputs": [],
      "source": [
        "def train_and_validate(model, train_loader, val_loader, optimizer, epochs, trial, model_artifact_name):\n",
        "    \"\"\"\n",
        "    Main function for training and validation loop.\n",
        "    This version includes an outer progress bar for epochs.\n",
        "    \"\"\"\n",
        "    best_val_f1 = 0.0\n",
        "    \n",
        "    # --- הוספת פס התקדמות חיצוני לאפוקים ---\n",
        "    epoch_progress_bar = tqdm(range(1, epochs + 1), desc=f\"Trial {trial.number} Epochs\")\n",
        "    \n",
        "    for epoch in epoch_progress_bar:\n",
        "        # --- עדכון התיאור של פס ההתקדמות ---\n",
        "        epoch_progress_bar.set_description(f\"Trial {trial.number} - Epoch {epoch}/{epochs}\")\n",
        "\n",
        "        # --- Training Loop (inner progress bar) ---\n",
        "        model.train()\n",
        "        train_loader_tqdm = tqdm(train_loader, desc=\"Training\", leave=False)\n",
        "        for batch in train_loader_tqdm:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(\n",
        "                input_ids=batch['input_ids'].to(device),\n",
        "                attention_mask=batch['attention_mask'].to(device),\n",
        "                labels=batch['labels'].to(device)\n",
        "            )\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loader_tqdm.set_postfix(loss=loss.item())\n",
        "\n",
        "        # --- Validation Loop (inner progress bar) ---\n",
        "        model.eval()\n",
        "        all_val_labels, all_val_preds = [], []\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader, desc=\"Validation\", leave=False):\n",
        "                outputs = model(\n",
        "                    input_ids=batch['input_ids'].to(device),\n",
        "                    attention_mask=batch['attention_mask'].to(device)\n",
        "                )\n",
        "                preds = outputs.logits.argmax(dim=1)\n",
        "                all_val_labels.extend(batch['labels'].cpu().numpy())\n",
        "                all_val_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "        val_f1 = f1_score(all_val_labels, all_val_preds, average='weighted', zero_division=0)\n",
        "        wandb.log({\"Epoch\": epoch, \"Validation F1\": val_f1})\n",
        "        \n",
        "        # Update the outer progress bar's postfix with the latest F1 score\n",
        "        epoch_progress_bar.set_postfix(val_f1=f\"{val_f1:.4f}\")\n",
        "        \n",
        "        # --- Pruning Logic ---\n",
        "        trial.report(val_f1, epoch)\n",
        "        if trial.should_prune():\n",
        "            del model, train_loader, val_loader, optimizer\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "            raise optuna.TrialPruned()\n",
        "        \n",
        "        if val_f1 > best_val_f1:\n",
        "            print(f\"🚀 New best model found! F1: {val_f1:.4f} (Epoch {epoch})\")\n",
        "            best_val_f1 = val_f1\n",
        "            artifact = wandb.Artifact(\n",
        "                name=model_artifact_name, type='model',\n",
        "                description=f'Best model from trial {trial.number} with F1: {val_f1:.4f}',\n",
        "                metadata=dict(trial.params, epoch=epoch, val_f1=val_f1)\n",
        "            )\n",
        "            with artifact.new_file(\"model.pt\", mode=\"wb\") as f:\n",
        "                torch.save(model.state_dict(), f)\n",
        "            wandb.log_artifact(artifact, aliases=['best', f'trial-{trial.number}'])\n",
        "            \n",
        "    return best_val_f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "kJJYD8lOXfno"
      },
      "outputs": [],
      "source": [
        "def objective(trial, model_name):\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True)\n",
        "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-7, 0.1, log=True)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32])\n",
        "    num_unfreeze_layers = trial.suggest_int(\"num_unfreeze_layers\", 1, 4)\n",
        "\n",
        "    model_short_name = model_name.split('/')[-1]\n",
        "    config = trial.params\n",
        "    config['model_name'] = model_name # Add model_name to config\n",
        "\n",
        "    wandb.init(\n",
        "        project=\"moti-matan-tel-aviv-university\",\n",
        "        config=config,\n",
        "        name=f\"manual-{model_short_name}-trial-{trial.number}\",\n",
        "        reinit=True,\n",
        "        group=f\"Optuna-{model_short_name}\"\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5, ignore_mismatched_sizes=True).to(device)\n",
        "\n",
        "    # Layer Freezing\n",
        "    for param in model.base_model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    encoder_layers = list(model.base_model.encoder.layer)\n",
        "    for layer in encoder_layers[-num_unfreeze_layers:]:\n",
        "        for param in layer.parameters():\n",
        "            param.requires_grad = True\n",
        "    for param in model.classifier.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    train_subset_df = train_df.sample(frac=0.3, random_state=42)\n",
        "    train_dataset = TweetsDataset(train_subset_df, tokenizer)\n",
        "    val_dataset = TweetsDataset(val_df, tokenizer)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "    best_f1 = train_and_validate(model, train_loader, val_loader, optimizer, epochs=5, trial=trial, model_artifact_name=f\"manual-{model_short_name}\")\n",
        "\n",
        "    wandb.finish()\n",
        "    del model, tokenizer, train_loader, val_loader, optimizer\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return best_f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "hs92TdQ8XfqQ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-09 07:10:27,572] A new study created in memory with name: no-name-83523677-886e-4d16-9617-665d88e8fb77\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Starting Optuna study for cardiffnlp/twitter-roberta-base-sentiment ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/data/wandb/wandb/run-20250809_071027-jyrhodv7</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/jyrhodv7' target=\"_blank\">manual-twitter-roberta-base-sentiment-trial-0</a></strong> to <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/jyrhodv7' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/jyrhodv7</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment and are newly initialized because the shapes did not match:\n",
            "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
            "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "61fba3c0d1104185ae2dc168235b7087",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Trial 0 Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f640f665cdda40abb42762a6e6925249",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fc1c45a06b51467aa56a7f9f7e1a222c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5062 (Epoch 1)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7f851e7ed29949e68f47a0d71b30f2c1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "458fc78e64c045718688365c3eab7837",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5833 (Epoch 2)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "422a87ac08be42a4b1c6962e41bec784",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cf9e60b8239a4aac9a718baacd215b1f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5903 (Epoch 3)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "16ebfc52cc3d4677b6ee6732cce9de01",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7ac8aa5e88b244d89849bfeef05a3c2d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.6084 (Epoch 4)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "88fb44b6c01440e181f8a7fbba6f43e8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "89b90e5455fc4ba49bfc3635d798961e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.6313 (Epoch 5)\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▃▅▆█</td></tr><tr><td>Validation F1</td><td>▁▅▆▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Validation F1</td><td>0.63131</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">manual-twitter-roberta-base-sentiment-trial-0</strong> at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/jyrhodv7' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/jyrhodv7</a><br> View project at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a><br>Synced 5 W&B file(s), 0 media file(s), 10 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>/data/wandb/wandb/run-20250809_071027-jyrhodv7/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-09 07:23:45,746] Trial 0 finished with value: 0.6313095107620008 and parameters: {'learning_rate': 3.602978435454494e-05, 'weight_decay': 5.06165052892724e-06, 'batch_size': 16, 'num_unfreeze_layers': 3}. Best is trial 0 with value: 0.6313095107620008.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/data/wandb/wandb/run-20250809_072345-vyj265rs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/vyj265rs' target=\"_blank\">manual-twitter-roberta-base-sentiment-trial-1</a></strong> to <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/vyj265rs' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/vyj265rs</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment and are newly initialized because the shapes did not match:\n",
            "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
            "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5a33eae6b3854d7bbcb31e5ba33973fd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Trial 1 Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "11d518528975488f8e601da2186641ef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1580dffb89f74efda79e7ce10cedbade",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5505 (Epoch 1)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9208c37e22284004993d5b6757d4697e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cb3e0b4b023c48eaa597f3f577731676",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5708 (Epoch 2)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "34fc65e82afe4b5ebf5baf881d01a9a8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "95fa9d10119f47c493890ebbb562ab58",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.6067 (Epoch 3)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c6fa24bc6c9c46dfb1995b361b99dbea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "239c7b0d54c3433c8256db955df2c372",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.6262 (Epoch 4)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ab00a76089a4ff2b1f70aed7bf76bdf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb1dd5b561534a4c9162af020c2fb47a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▃▅▆█</td></tr><tr><td>Validation F1</td><td>▁▃▆█▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Validation F1</td><td>0.61196</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">manual-twitter-roberta-base-sentiment-trial-1</strong> at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/vyj265rs' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/vyj265rs</a><br> View project at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a><br>Synced 5 W&B file(s), 0 media file(s), 8 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>/data/wandb/wandb/run-20250809_072345-vyj265rs/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-09 07:36:50,985] Trial 1 finished with value: 0.626160335915341 and parameters: {'learning_rate': 3.635357693684378e-05, 'weight_decay': 4.774943948309924e-07, 'batch_size': 16, 'num_unfreeze_layers': 3}. Best is trial 0 with value: 0.6313095107620008.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/data/wandb/wandb/run-20250809_073650-3qd5ni33</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/3qd5ni33' target=\"_blank\">manual-twitter-roberta-base-sentiment-trial-2</a></strong> to <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/3qd5ni33' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/3qd5ni33</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment and are newly initialized because the shapes did not match:\n",
            "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
            "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ced7a7816e574e57add764c5a77b675e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Trial 2 Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "845db09824d1448d8fef9ec42a791066",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d38ba8ce87124bafbf3f267cf277632d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5137 (Epoch 1)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "485654cf7ad34aac83c1edd817f0b098",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f74cc28a7645484f8939dca9679ac0d1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5498 (Epoch 2)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "810b19fb8684474bb1b2f7934f4535de",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b578dbaf5b4148bcab566ff9456c2e82",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5803 (Epoch 3)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a2e4a04a75dd43afabecd00eb805bde5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8a5ce059aae8436daf4966fd5c59f819",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.6028 (Epoch 4)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8a639021d5cf4cc3bb0b17bed6f6b30e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cdfb734e0fcf4898a42e3af92b18d569",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▃▅▆█</td></tr><tr><td>Validation F1</td><td>▁▄▆█▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Validation F1</td><td>0.57861</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">manual-twitter-roberta-base-sentiment-trial-2</strong> at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/3qd5ni33' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/3qd5ni33</a><br> View project at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a><br>Synced 5 W&B file(s), 0 media file(s), 8 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>/data/wandb/wandb/run-20250809_073650-3qd5ni33/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-09 07:49:55,828] Trial 2 finished with value: 0.6027993159904803 and parameters: {'learning_rate': 1.6256757700109393e-05, 'weight_decay': 0.003996165239403762, 'batch_size': 16, 'num_unfreeze_layers': 3}. Best is trial 0 with value: 0.6313095107620008.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/data/wandb/wandb/run-20250809_074955-rm2yn02i</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/rm2yn02i' target=\"_blank\">manual-twitter-roberta-base-sentiment-trial-3</a></strong> to <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/rm2yn02i' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/rm2yn02i</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment and are newly initialized because the shapes did not match:\n",
            "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
            "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e1b2c7aa09f45c3bb3add5000dfe2e6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Trial 3 Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6e0aca67f4a249bb95473ec619994f17",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/309 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7750cc6ba0ce4b0b8e4a5d1b5007ee22",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/258 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.4302 (Epoch 1)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9823c91d98ea48c98c036f98cfbb3838",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/309 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2b4cb485e3c8400f8dcae944ece489d3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/258 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.4720 (Epoch 2)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f014a4cdd5784c14a1d16d38be92af75",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/309 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cdff30f83fc548b0ab5bd26f21feca4a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/258 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.4898 (Epoch 3)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "19a6ba44998545c5971c47a1d1bb132b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/309 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "24a20ef6610f48a4990beebcf42a30fa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/258 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5031 (Epoch 4)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "12089658283648a9b002ac6e2779cb15",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/309 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e05afb6ddb464f46b9644f0870e0bea5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/258 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5285 (Epoch 5)\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▃▅▆█</td></tr><tr><td>Validation F1</td><td>▁▄▅▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Validation F1</td><td>0.52847</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">manual-twitter-roberta-base-sentiment-trial-3</strong> at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/rm2yn02i' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/rm2yn02i</a><br> View project at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a><br>Synced 5 W&B file(s), 0 media file(s), 10 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>/data/wandb/wandb/run-20250809_074955-rm2yn02i/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-09 08:00:42,410] Trial 3 finished with value: 0.5284748261977539 and parameters: {'learning_rate': 3.1256493649358045e-05, 'weight_decay': 0.00032323590769821347, 'batch_size': 32, 'num_unfreeze_layers': 1}. Best is trial 0 with value: 0.6313095107620008.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/data/wandb/wandb/run-20250809_080042-vv4yuhhf</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/vv4yuhhf' target=\"_blank\">manual-twitter-roberta-base-sentiment-trial-4</a></strong> to <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/vv4yuhhf' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/vv4yuhhf</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment and are newly initialized because the shapes did not match:\n",
            "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
            "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c9766f0eea8943a0ab400a615bdbcb4c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Trial 4 Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "85bc55d0d9b54804affe714ac906d06a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8a9fb01879e5445f98984b7afde3a7b7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.4461 (Epoch 1)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0cb48114c12344b6a34990bd2a45665f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7bd135d10df340bfa8504b2a051c7207",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.4971 (Epoch 2)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c181d22439241c881808ddd1d306228",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4c525356b4344b4d8dd2d71db2005f2a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5251 (Epoch 3)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a21d3f29970c4e0b80729ddb8afb30f3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ce6105e32acb4b35856e92bae5d0fd3e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5417 (Epoch 4)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fe2c5df8d3b646c981e27ccadee35476",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "17f5d9755d6b4c9e9732783c46dbfa9a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5461 (Epoch 5)\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▃▅▆█</td></tr><tr><td>Validation F1</td><td>▁▅▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Validation F1</td><td>0.54611</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">manual-twitter-roberta-base-sentiment-trial-4</strong> at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/vv4yuhhf' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/vv4yuhhf</a><br> View project at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a><br>Synced 5 W&B file(s), 0 media file(s), 10 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>/data/wandb/wandb/run-20250809_080042-vv4yuhhf/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-09 08:12:12,301] Trial 4 finished with value: 0.5461139206600677 and parameters: {'learning_rate': 4.033259035390765e-05, 'weight_decay': 2.0488008496575643e-05, 'batch_size': 16, 'num_unfreeze_layers': 1}. Best is trial 0 with value: 0.6313095107620008.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/data/wandb/wandb/run-20250809_081212-lr6lhv47</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/lr6lhv47' target=\"_blank\">manual-twitter-roberta-base-sentiment-trial-5</a></strong> to <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/lr6lhv47' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/lr6lhv47</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment and are newly initialized because the shapes did not match:\n",
            "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
            "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ec6d6ecfe74547ada36d0eb1cb4fe297",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Trial 5 Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a707a91188c4c3088816c638a1c9711",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "198538e3fa6044768964699738c1d829",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5343 (Epoch 1)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f1c395114334fe18781e25884382739",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e04efc6c2304d6fa2a899c26dc5e967",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5584 (Epoch 2)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3fd61f6add214d5e89412434bd47dbec",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a1444361e974d89891e041183fb376c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5998 (Epoch 3)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f84ab88c7f6a4bdfbeb73feb87ba8373",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bd4edfa8efa941e1a150299b4836d534",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.6156 (Epoch 4)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3f1442ef05154dc98aafac599193e8fd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e853bb75e869488682638452d450741d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▃▅▆█</td></tr><tr><td>Validation F1</td><td>▁▃▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Validation F1</td><td>0.61064</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">manual-twitter-roberta-base-sentiment-trial-5</strong> at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/lr6lhv47' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/lr6lhv47</a><br> View project at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a><br>Synced 5 W&B file(s), 0 media file(s), 8 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>/data/wandb/wandb/run-20250809_081212-lr6lhv47/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-09 08:25:17,031] Trial 5 finished with value: 0.6155614373069268 and parameters: {'learning_rate': 2.1846952565174106e-05, 'weight_decay': 0.0014649081936008194, 'batch_size': 16, 'num_unfreeze_layers': 3}. Best is trial 0 with value: 0.6313095107620008.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/data/wandb/wandb/run-20250809_082517-o65gy8gy</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/o65gy8gy' target=\"_blank\">manual-twitter-roberta-base-sentiment-trial-6</a></strong> to <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/o65gy8gy' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/o65gy8gy</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment and are newly initialized because the shapes did not match:\n",
            "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
            "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c9cdc6337ec64fccafe592ce86a5fced",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Trial 6 Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c1317f0499e647fa8fb5492d85e006b0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/309 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3dfacbb806014a38b2357da4e917ad97",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/258 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-09 08:27:32,959] Trial 6 pruned. \n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to True."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁</td></tr><tr><td>Validation F1</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>1</td></tr><tr><td>Validation F1</td><td>0.48094</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">manual-twitter-roberta-base-sentiment-trial-6</strong> at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/o65gy8gy' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/o65gy8gy</a><br> View project at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>/data/wandb/wandb/run-20250809_082517-o65gy8gy/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/data/wandb/wandb/run-20250809_082732-l66sa5w7</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/l66sa5w7' target=\"_blank\">manual-twitter-roberta-base-sentiment-trial-7</a></strong> to <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/l66sa5w7' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/l66sa5w7</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment and are newly initialized because the shapes did not match:\n",
            "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
            "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1b014b38ad924d01b46160c50db543ea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Trial 7 Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2101aa0dece549f0ba7a7087634c6f99",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0acbcd63fc44684836a4284198eb510",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-09 08:29:58,416] Trial 7 pruned. \n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to True."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁</td></tr><tr><td>Validation F1</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>1</td></tr><tr><td>Validation F1</td><td>0.4901</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">manual-twitter-roberta-base-sentiment-trial-7</strong> at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/l66sa5w7' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/l66sa5w7</a><br> View project at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>/data/wandb/wandb/run-20250809_082732-l66sa5w7/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/data/wandb/wandb/run-20250809_082958-8tu8dn1v</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/8tu8dn1v' target=\"_blank\">manual-twitter-roberta-base-sentiment-trial-8</a></strong> to <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/8tu8dn1v' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/8tu8dn1v</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment and are newly initialized because the shapes did not match:\n",
            "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
            "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dfbd210076c544d9b018e4250026d255",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Trial 8 Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "51e5e9920d5549b39541d7ca72f0335f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/309 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6783ab6596904d1392b1f9d277a271ef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/258 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-09 08:32:25,121] Trial 8 pruned. \n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to True."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁</td></tr><tr><td>Validation F1</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>1</td></tr><tr><td>Validation F1</td><td>0.48725</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">manual-twitter-roberta-base-sentiment-trial-8</strong> at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/8tu8dn1v' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/8tu8dn1v</a><br> View project at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>/data/wandb/wandb/run-20250809_082958-8tu8dn1v/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/data/wandb/wandb/run-20250809_083225-dvisd4te</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/dvisd4te' target=\"_blank\">manual-twitter-roberta-base-sentiment-trial-9</a></strong> to <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/dvisd4te' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/dvisd4te</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment and are newly initialized because the shapes did not match:\n",
            "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
            "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "878c91f941834e389002716955f3259b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Trial 9 Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0fc2ef84d20f4c82886309c3ab5a23a1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/309 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb426794bbb64ac0b6900ac04ceaae59",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/258 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5106 (Epoch 1)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cf241c97bb854a11a7a8e072b5d191b6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/309 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "651fa0c300814aa68ebb0b6d32c2af4a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/258 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5610 (Epoch 2)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "28470b83ad874f868b05e92e4a8bbcb0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/309 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6849863cde0b42a7a2979c309364e2b4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/258 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5856 (Epoch 3)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e71380452e6b4fbe81d5f3adc6e4daac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/309 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "59f5a887b9ff4b12a01febd7daf18d2c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/258 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-09 08:42:55,785] Trial 9 pruned. \n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to True."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▃▆█</td></tr><tr><td>Validation F1</td><td>▁▆██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>4</td></tr><tr><td>Validation F1</td><td>0.58182</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">manual-twitter-roberta-base-sentiment-trial-9</strong> at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/dvisd4te' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/dvisd4te</a><br> View project at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a><br>Synced 5 W&B file(s), 0 media file(s), 6 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>/data/wandb/wandb/run-20250809_083225-dvisd4te/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/data/wandb/wandb/run-20250809_084255-n3hpauu0</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/n3hpauu0' target=\"_blank\">manual-twitter-roberta-base-sentiment-trial-10</a></strong> to <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/n3hpauu0' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/n3hpauu0</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment and are newly initialized because the shapes did not match:\n",
            "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
            "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a4f45fb430664ee09f6dc6ad5d230702",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Trial 10 Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5efd916c3a904a03b5d1d5d4424d1e06",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d5c44c3aafa343bdbde810ef54b35eb8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5797 (Epoch 1)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5d98b5687b0246b583fff92eaf3627a4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "64d9b9ad429a4e6caffd15ec03d48bcf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.6307 (Epoch 2)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fc30ff779b19492eae0e8fd44f5e40fd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9b35a2e0a45f43eda10bf0ea4858d574",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.6447 (Epoch 3)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "511ddf00c6cb4e99a01e8411cb9fbb45",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d576fe872c384dedaf1c64e501e165b3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.6500 (Epoch 4)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "50f6134839bc497a9f323ec0acc5ee1a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a4321a13a79247c492fd9639186cd500",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▃▅▆█</td></tr><tr><td>Validation F1</td><td>▁▆▇█▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Validation F1</td><td>0.64021</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">manual-twitter-roberta-base-sentiment-trial-10</strong> at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/n3hpauu0' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/n3hpauu0</a><br> View project at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a><br>Synced 5 W&B file(s), 0 media file(s), 8 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>/data/wandb/wandb/run-20250809_084255-n3hpauu0/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-09 08:56:53,561] Trial 10 finished with value: 0.6499960574725696 and parameters: {'learning_rate': 4.584013354323171e-05, 'weight_decay': 0.07357512825552659, 'batch_size': 16, 'num_unfreeze_layers': 4}. Best is trial 10 with value: 0.6499960574725696.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/data/wandb/wandb/run-20250809_085653-ez1v5meb</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/ez1v5meb' target=\"_blank\">manual-twitter-roberta-base-sentiment-trial-11</a></strong> to <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/ez1v5meb' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/ez1v5meb</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment and are newly initialized because the shapes did not match:\n",
            "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
            "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ea9b5357d8bb430f8be9ea471b25911f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Trial 11 Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "36a3aa5feae54d5eb1ab8d188a374d30",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f491d66dfe8e43f4b1a68dea3c747d33",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5734 (Epoch 1)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3ed6087b081b4cfea02648d6fe97120b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "61df7201cf2245b0a6df6adcf2c6ce8e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.6324 (Epoch 2)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d24e5a77c1474d3c8291ea670e3f9c91",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b7afab963cb64dc9bd29b614dd2c602b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee85a651fbfc44808e041d2eb0c3708c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e848f2f60a124d93b8cd149cd82be342",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.6381 (Epoch 4)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "876059adf86e451ba4ded67420db96f2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "29a9b107001b409f84c028495f202d39",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.6488 (Epoch 5)\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▃▅▆█</td></tr><tr><td>Validation F1</td><td>▁▆▅▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Validation F1</td><td>0.64877</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">manual-twitter-roberta-base-sentiment-trial-11</strong> at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/ez1v5meb' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/ez1v5meb</a><br> View project at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a><br>Synced 5 W&B file(s), 0 media file(s), 8 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>/data/wandb/wandb/run-20250809_085653-ez1v5meb/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-09 09:10:57,730] Trial 11 finished with value: 0.6487654929617506 and parameters: {'learning_rate': 4.782890276843486e-05, 'weight_decay': 0.08944919072752507, 'batch_size': 16, 'num_unfreeze_layers': 4}. Best is trial 10 with value: 0.6499960574725696.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best trial for twitter-roberta-base-sentiment: 0.6499960574725696\n",
            "Best parameters: {'learning_rate': 4.584013354323171e-05, 'weight_decay': 0.07357512825552659, 'batch_size': 16, 'num_unfreeze_layers': 4}\n"
          ]
        }
      ],
      "source": [
        "# --- Model 1: cardiffnlp/twitter-roberta-base-sentiment ---\n",
        "print(\"--- Starting Optuna study for cardiffnlp/twitter-roberta-base-sentiment ---\")\n",
        "study_roberta = optuna.create_study(direction=\"maximize\")\n",
        "study_roberta.optimize(lambda trial: objective(trial, model_name=\"cardiffnlp/twitter-roberta-base-sentiment\"), n_trials=12)\n",
        "print(f\"Best trial for twitter-roberta-base-sentiment: {study_roberta.best_trial.value}\")\n",
        "print(f\"Best parameters: {study_roberta.best_trial.params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mdb_C-9eXfs6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-09 09:10:57,992] A new study created in memory with name: no-name-cc3f721f-d935-4793-a3b8-f4e639c1e699\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Starting Optuna study for roberta-base ---\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/data/wandb/wandb/run-20250809_091057-gwrcj7a5</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/gwrcj7a5' target=\"_blank\">manual-roberta-base-trial-0</a></strong> to <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/gwrcj7a5' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/gwrcj7a5</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9bc16ecc6a444e7baa80caf22adc57d7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "17525056c0b2464da575d3230c10ac92",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "81864834a7ff4f5394027ef02bdc8447",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "15ae7ec28d7245db81612e7ad5cbd0c2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "432cb20349784e728d63c7bac1e4299e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "259c3443ea774620b939b42f773b27f1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "284bbed46dd4424f9321cad4bab1a68b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Trial 0 Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b9338aaf4a5b4d8fb19c4f4a8c6676fe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/309 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "714461990621406c952821535b8a443e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/258 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.4480 (Epoch 1)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d834b94986a44dc3b617f2d232e7d3ff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/309 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "25f240e91b7a4fcf814f0652e55acc96",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/258 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5133 (Epoch 2)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8eb92aa8ff26491f99cef93e801a8a66",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/309 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "72eac247912643da92e739586c6c5839",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/258 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5214 (Epoch 3)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e1fce7656457433e9d2c49327caf5095",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/309 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ec705d822d324600b33559161e68bd03",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/258 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5519 (Epoch 4)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a18351a747294ad7a6bc5547c245be45",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/309 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "338d20ee634e4e92adf6063dc8872826",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/258 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5680 (Epoch 5)\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▃▅▆█</td></tr><tr><td>Validation F1</td><td>▁▅▅▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Validation F1</td><td>0.56803</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">manual-roberta-base-trial-0</strong> at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/gwrcj7a5' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/gwrcj7a5</a><br> View project at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a><br>Synced 5 W&B file(s), 0 media file(s), 10 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>/data/wandb/wandb/run-20250809_091057-gwrcj7a5/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-09 09:23:26,487] Trial 0 finished with value: 0.5680272718070384 and parameters: {'learning_rate': 1.0397713196315484e-05, 'weight_decay': 1.2331674571361498e-07, 'batch_size': 32, 'num_unfreeze_layers': 3}. Best is trial 0 with value: 0.5680272718070384.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/data/wandb/wandb/run-20250809_092326-i2ompya8</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/i2ompya8' target=\"_blank\">manual-roberta-base-trial-1</a></strong> to <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/i2ompya8' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/i2ompya8</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "92c6e337de6c4537a4a272f1b0f7d2f4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Trial 1 Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7e5a2c88c39646c7a424ae4fa937c28f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/309 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b8c4be0e935e482ca1fde5b498282070",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/258 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.3191 (Epoch 1)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "265d1031526e48f089abb6b859271240",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/309 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "49565273fe824448922fd19c34c2ae81",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/258 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.4556 (Epoch 2)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e7aeca4ef2314955949018e823592fa1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/309 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f7482e3c96f34f31aa8136256802cb21",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/258 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.4837 (Epoch 3)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "adc63ba8519d42529c00f33f7f891391",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/309 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2a5454dd0bbe4438a1e27ad45b640c1f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/258 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5037 (Epoch 4)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ed088adaa3a46e48010e8cead77f958",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/309 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1f9d5375b2d141eb8c0f551cfd6896e6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/258 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5115 (Epoch 5)\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▃▅▆█</td></tr><tr><td>Validation F1</td><td>▁▆▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Validation F1</td><td>0.5115</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">manual-roberta-base-trial-1</strong> at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/i2ompya8' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/i2ompya8</a><br> View project at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a><br>Synced 5 W&B file(s), 0 media file(s), 10 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>/data/wandb/wandb/run-20250809_092326-i2ompya8/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-09 09:34:10,656] Trial 1 finished with value: 0.511497924391657 and parameters: {'learning_rate': 1.847125941484408e-05, 'weight_decay': 1.1816346173642974e-06, 'batch_size': 32, 'num_unfreeze_layers': 1}. Best is trial 0 with value: 0.5680272718070384.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/data/wandb/wandb/run-20250809_093410-cu7palo3</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/cu7palo3' target=\"_blank\">manual-roberta-base-trial-2</a></strong> to <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/cu7palo3' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/cu7palo3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e73f037dc4da419f8b4717d82cdd0ad3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Trial 2 Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1edf049c3cac411b9f05a1a7252e4004",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/309 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5b263dd2587f4400915336e9af84bce4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/258 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.3670 (Epoch 1)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "27d1524b7de241fca8f218ced9fbed42",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/309 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "211954f46d1c45a5aebf0876fcf60443",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/258 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.4629 (Epoch 2)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3943fa5b4d7744159381e00bd1ccdf13",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/309 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "338efb29066c4d2e89e529e9539d6b09",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/258 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.4906 (Epoch 3)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b1222d2bfc6b4a448d1d438caf9f73a7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/309 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "786433b3b60a400e8bb3b88dece6985a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/258 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5096 (Epoch 4)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0a499cb4e7d548aaae6437089c1ac670",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/309 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a8dfe0c87d564e228447448d94f271c7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/258 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5222 (Epoch 5)\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▃▅▆█</td></tr><tr><td>Validation F1</td><td>▁▅▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Validation F1</td><td>0.52224</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">manual-roberta-base-trial-2</strong> at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/cu7palo3' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/cu7palo3</a><br> View project at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a><br>Synced 5 W&B file(s), 0 media file(s), 10 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>/data/wandb/wandb/run-20250809_093410-cu7palo3/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-09 09:44:53,536] Trial 2 finished with value: 0.5222353197293785 and parameters: {'learning_rate': 2.1644511042858013e-05, 'weight_decay': 0.008112133938337044, 'batch_size': 32, 'num_unfreeze_layers': 1}. Best is trial 0 with value: 0.5680272718070384.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/data/wandb/wandb/run-20250809_094453-emvgp5fr</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/emvgp5fr' target=\"_blank\">manual-roberta-base-trial-3</a></strong> to <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/emvgp5fr' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/emvgp5fr</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7a7c96c560544d4e9cf04c7a7e2d1d05",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Trial 3 Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4990ddd573a34dbb9872bb39f0d1cdf0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/309 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "876a4c9bc2b747deb0946cb7874c4a86",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/258 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5070 (Epoch 1)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb486786117d4d86ab0d0124532ff602",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/309 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "63080a85250e4802bac407b6fc6f8b3a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/258 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5542 (Epoch 2)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d6edaceb75f1474d8d5a916bb476613d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/309 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "84238a5ffb9448be9d9705c2fe989174",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/258 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5807 (Epoch 3)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9b9878bed3f343f483f10fa420fd80c1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/309 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5702b302bc86420e8ee52e46eccdf370",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/258 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5851 (Epoch 4)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6064ef63e6a34f3bb44ebf84c43aa9ff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/309 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a6f7dd1fd99546f8a36bf682a17e71e7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/258 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5909 (Epoch 5)\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▃▅▆█</td></tr><tr><td>Validation F1</td><td>▁▅▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Validation F1</td><td>0.59094</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">manual-roberta-base-trial-3</strong> at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/emvgp5fr' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/emvgp5fr</a><br> View project at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a><br>Synced 5 W&B file(s), 0 media file(s), 10 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>/data/wandb/wandb/run-20250809_094453-emvgp5fr/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-09 09:56:27,364] Trial 3 finished with value: 0.5909436959756996 and parameters: {'learning_rate': 4.7929901289161835e-05, 'weight_decay': 1.3282074455042119e-06, 'batch_size': 32, 'num_unfreeze_layers': 2}. Best is trial 3 with value: 0.5909436959756996.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/data/wandb/wandb/run-20250809_095627-1fpow8l3</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/1fpow8l3' target=\"_blank\">manual-roberta-base-trial-4</a></strong> to <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/1fpow8l3' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/1fpow8l3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c48fd34d174a4e0db5ccbf3dcf7d4c20",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Trial 4 Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8a8308e50aa14ffba363e3916480562a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/309 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0dc983f4ccfd47ba80b0a1338e9187e5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/258 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.4097 (Epoch 1)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f7b55f526a9f4a13a441c895f3e15c2e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/309 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c8f4b10841d9407a9e0b832fdb28dc65",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/258 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.4763 (Epoch 2)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "21a3410396604fdb8ca21b6d954dbf07",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/309 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a10e66b9a33c4a6b9de69606b66cc67b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/258 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5003 (Epoch 3)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ff090c1611f4544b75253bf76d04658",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/309 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "97df0cab7f3c4e3292afab17ded09443",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/258 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5125 (Epoch 4)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "08340f6c5c674f91894807bb9eef0969",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/309 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5fcfcb752af44ea5a8f20af4bbe5c6da",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/258 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5237 (Epoch 5)\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▃▅▆█</td></tr><tr><td>Validation F1</td><td>▁▅▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Validation F1</td><td>0.52366</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">manual-roberta-base-trial-4</strong> at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/1fpow8l3' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/1fpow8l3</a><br> View project at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a><br>Synced 5 W&B file(s), 0 media file(s), 10 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>/data/wandb/wandb/run-20250809_095627-1fpow8l3/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-09 10:07:09,996] Trial 4 finished with value: 0.5236636620044861 and parameters: {'learning_rate': 2.898650203441786e-05, 'weight_decay': 0.04908383211047201, 'batch_size': 32, 'num_unfreeze_layers': 1}. Best is trial 3 with value: 0.5909436959756996.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/data/wandb/wandb/run-20250809_100709-ofrq73l3</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/ofrq73l3' target=\"_blank\">manual-roberta-base-trial-5</a></strong> to <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/ofrq73l3' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/ofrq73l3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c0ac0442b1f24ce1a1a478ca3cf706db",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Trial 5 Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "78d68dcce3a14cf3a1eb10d522439522",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/309 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "99c63ed863624f419f8dff21367e40b4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/258 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5220 (Epoch 1)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1832d518d5c44e37aea8c109eec68650",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/309 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1a19bea598a74eb6a09f2590893a7d7c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/258 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5675 (Epoch 2)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fe86182b0ce24671bc81583e1d9c354e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/309 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5cf7c55c926a444489335d23592384bb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/258 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5733 (Epoch 3)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a111784d82a84b5d88bb820b72c9a50b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/309 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "93ef4c2a72994033844db6a3669900eb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/258 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.6025 (Epoch 4)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c658f25590224c568bbff1c2f0b8954a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/309 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b3dbdcef41834e83a6fd8a374f568186",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/258 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▃▅▆█</td></tr><tr><td>Validation F1</td><td>▁▅▅█▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Validation F1</td><td>0.58487</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">manual-roberta-base-trial-5</strong> at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/ofrq73l3' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/ofrq73l3</a><br> View project at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a><br>Synced 5 W&B file(s), 0 media file(s), 8 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>/data/wandb/wandb/run-20250809_100709-ofrq73l3/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-09 10:19:23,064] Trial 5 finished with value: 0.6025306055153575 and parameters: {'learning_rate': 3.1212952317354524e-05, 'weight_decay': 0.046414420922664394, 'batch_size': 32, 'num_unfreeze_layers': 3}. Best is trial 5 with value: 0.6025306055153575.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/data/wandb/wandb/run-20250809_101923-0nkf7fn1</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/0nkf7fn1' target=\"_blank\">manual-roberta-base-trial-6</a></strong> to <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/0nkf7fn1' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/0nkf7fn1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "47bc647dfa69488bb14e692951ffc6aa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Trial 6 Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bfd344b2fdaa4efc85145548b567dc57",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "73610cf756304ae79ad493cdc9811fc3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5051 (Epoch 1)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "335fda022911494b98b02cbe0b581386",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a8aa42d1db5a4212bea7a2ae0828a6cc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5665 (Epoch 2)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9d56e8f5d02245ad9245e184d9c38216",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f928856941684bf3a3620a76e6d8c23e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5909 (Epoch 3)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d41cb8499c004fb1a3d2c6fbd0b57a2e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ddee8cec95034128a6da2737d3569c68",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "74be16f305e64e07904fc9684b3b105c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f5b5f39457854271a0c3a46617806e8f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5961 (Epoch 5)\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▃▅▆█</td></tr><tr><td>Validation F1</td><td>▁▆█▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Validation F1</td><td>0.59608</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">manual-roberta-base-trial-6</strong> at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/0nkf7fn1' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/0nkf7fn1</a><br> View project at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a><br>Synced 5 W&B file(s), 0 media file(s), 8 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>/data/wandb/wandb/run-20250809_101923-0nkf7fn1/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-09 10:32:32,840] Trial 6 finished with value: 0.5960806315903034 and parameters: {'learning_rate': 1.705079421574817e-05, 'weight_decay': 9.949476247942805e-06, 'batch_size': 16, 'num_unfreeze_layers': 3}. Best is trial 5 with value: 0.6025306055153575.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/data/wandb/wandb/run-20250809_103232-6lf5iyqb</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/6lf5iyqb' target=\"_blank\">manual-roberta-base-trial-7</a></strong> to <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/6lf5iyqb' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/6lf5iyqb</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f28925937a8447daa0e3147adc4b6016",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Trial 7 Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "400a83022ae24822b22abc472fe51830",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/309 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "efa3951d350e4548b080422b2e256b32",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/258 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5032 (Epoch 1)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0a54fd9c38da4425a1e99d0f7024200d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/309 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5d9aa758433d43da95e6746778939fa9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/258 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5559 (Epoch 2)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "83a14f95ec9d4773a0c5637824f727c5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/309 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "486890da4fa34ba08e4ebd5a61408ab2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/258 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5664 (Epoch 3)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1647b56d4b5446429e920efacaa1f585",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/309 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "62a906f94c0a4d2e8c36397fec9e2074",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/258 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5840 (Epoch 4)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0115ac57120f4ebcb64d601a9da7df59",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/309 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0b797e688ba04a0288ad6bd4329ddd62",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/258 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▃▅▆█</td></tr><tr><td>Validation F1</td><td>▁▆▆██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Validation F1</td><td>0.58123</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">manual-roberta-base-trial-7</strong> at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/6lf5iyqb' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/6lf5iyqb</a><br> View project at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a><br>Synced 5 W&B file(s), 0 media file(s), 8 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>/data/wandb/wandb/run-20250809_103232-6lf5iyqb/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-09 10:44:47,072] Trial 7 finished with value: 0.5839743679436532 and parameters: {'learning_rate': 2.520915028282754e-05, 'weight_decay': 3.161435172162095e-07, 'batch_size': 32, 'num_unfreeze_layers': 3}. Best is trial 5 with value: 0.6025306055153575.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/data/wandb/wandb/run-20250809_104447-zzxbbq4n</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/zzxbbq4n' target=\"_blank\">manual-roberta-base-trial-8</a></strong> to <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/zzxbbq4n' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/zzxbbq4n</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cb571b1390254d01b536da69e243e9cc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Trial 8 Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bf45251845b24c55bfcee01e67836b48",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a9a168c45e8b4185ba722002ddc4c191",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5281 (Epoch 1)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5f16f8b6e83348c89a3cba1372e58cb1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7148c437374e4c429f5b237a39456949",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5713 (Epoch 2)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "abef7a3dbcc44c23a566ae9112abdec3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f1282d144da44fba8ac017f58c28a9fc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.6025 (Epoch 3)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ef7bbec204f4220a7758e8927638bd7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "77252a67623b4f96a108321d6da513ce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.6151 (Epoch 4)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f19f005056324f3ea86a0626ac3d99f9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1f05829ef875493ba8e0f8ce3f06488b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▃▅▆█</td></tr><tr><td>Validation F1</td><td>▁▄▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Validation F1</td><td>0.61457</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">manual-roberta-base-trial-8</strong> at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/zzxbbq4n' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/zzxbbq4n</a><br> View project at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a><br>Synced 5 W&B file(s), 0 media file(s), 8 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>/data/wandb/wandb/run-20250809_104447-zzxbbq4n/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-09 10:57:48,529] Trial 8 finished with value: 0.6150582844160238 and parameters: {'learning_rate': 4.3535289049703364e-05, 'weight_decay': 0.01984568287057082, 'batch_size': 16, 'num_unfreeze_layers': 3}. Best is trial 8 with value: 0.6150582844160238.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/data/wandb/wandb/run-20250809_105748-c804t1i7</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/c804t1i7' target=\"_blank\">manual-roberta-base-trial-9</a></strong> to <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/c804t1i7' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/c804t1i7</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "16865b84e3e140d88d99471205ac6a06",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Trial 9 Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "91ae91e2742c4737b309325418c2b556",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fbcb752d8827415295cac5892a66df18",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-09 11:00:23,156] Trial 9 pruned. \n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to True."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁</td></tr><tr><td>Validation F1</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>1</td></tr><tr><td>Validation F1</td><td>0.49526</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">manual-roberta-base-trial-9</strong> at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/c804t1i7' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/c804t1i7</a><br> View project at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>/data/wandb/wandb/run-20250809_105748-c804t1i7/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/data/wandb/wandb/run-20250809_110023-cfl43uv7</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/cfl43uv7' target=\"_blank\">manual-roberta-base-trial-10</a></strong> to <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/cfl43uv7' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/cfl43uv7</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fa42ed29fb3f4575936916b10ea24223",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Trial 10 Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c320e105c37c4ee8b122fdc69c059af9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "df769c24592a46868ce9b4ba9f3388ba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5659 (Epoch 1)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ac4c0cf6b02842368786a5a618613d92",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1b57a9e750da49e58decc4280c305484",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5986 (Epoch 2)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d1f483756f3844189784eafd7ae43d0e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "de9f5f499d12484d8616434bfeb1f571",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.6022 (Epoch 3)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d4682df5095e4be9811787ece5120334",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e64572084ae34353a11cc2f7298d51b6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7e9895f881ac488d8cc5519b391a2514",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bbc085ea4b064964bfc0b3d10f8dde40",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.6167 (Epoch 5)\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▃▅▆█</td></tr><tr><td>Validation F1</td><td>▁▅▆▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Validation F1</td><td>0.61673</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">manual-roberta-base-trial-10</strong> at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/cfl43uv7' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/cfl43uv7</a><br> View project at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a><br>Synced 5 W&B file(s), 0 media file(s), 8 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>/data/wandb/wandb/run-20250809_110023-cfl43uv7/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-09 11:14:25,642] Trial 10 finished with value: 0.6167294556677039 and parameters: {'learning_rate': 4.7880923166976995e-05, 'weight_decay': 0.000633976505241131, 'batch_size': 16, 'num_unfreeze_layers': 4}. Best is trial 10 with value: 0.6167294556677039.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/data/wandb/wandb/run-20250809_111425-4pr9e7mw</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/4pr9e7mw' target=\"_blank\">manual-roberta-base-trial-11</a></strong> to <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/4pr9e7mw' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/4pr9e7mw</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "41208c8fa3a84f8fac2da6c19bd4be1c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Trial 11 Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a51166d7c2a44c9983c1d81464d8bcbe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "683a29bc0868408097595af31dc8d85e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5760 (Epoch 1)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "845ffa8f22eb4ac59033ec71dce38082",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "da4076f03f1e4abaa034c07f4424fa21",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.5802 (Epoch 2)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a3d46a63f61f4a5e818ada297bcbe4c4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1182af2047b6475db14b6dbe5af31851",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.6006 (Epoch 3)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b681cbd491bd4d1d869790fdb127c0e1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0d98a64b4a3b4249a4f08eb7845fb4bd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.6166 (Epoch 4)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cef5e10aff0c4fccb16a6f537eb31de5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/618 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cafcfa876dcb46edbb2837f71c35b5f8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 New best model found! F1: 0.6170 (Epoch 5)\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▃▅▆█</td></tr><tr><td>Validation F1</td><td>▁▂▅██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Validation F1</td><td>0.61699</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">manual-roberta-base-trial-11</strong> at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/4pr9e7mw' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/4pr9e7mw</a><br> View project at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a><br>Synced 5 W&B file(s), 0 media file(s), 10 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>/data/wandb/wandb/run-20250809_111425-4pr9e7mw/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-09 11:28:29,910] Trial 11 finished with value: 0.6169873961881869 and parameters: {'learning_rate': 4.862140955744093e-05, 'weight_decay': 0.0013571248930636039, 'batch_size': 16, 'num_unfreeze_layers': 4}. Best is trial 11 with value: 0.6169873961881869.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best trial for roberta-base: 0.6169873961881869\n",
            "Best parameters: {'learning_rate': 4.862140955744093e-05, 'weight_decay': 0.0013571248930636039, 'batch_size': 16, 'num_unfreeze_layers': 4}\n"
          ]
        }
      ],
      "source": [
        "# --- Model 2: roberta-base ---\n",
        "print(\"\\n--- Starting Optuna study for roberta-base ---\")\n",
        "study_roberta_base = optuna.create_study(direction=\"maximize\")\n",
        "study_roberta_base.optimize(lambda trial: objective(trial, model_name=\"roberta-base\"), n_trials=12)\n",
        "print(f\"Best trial for roberta-base: {study_roberta_base.best_trial.value}\")\n",
        "print(f\"Best parameters: {study_roberta_base.best_trial.params}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_c_PmQT0XgAQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Training cardiffnlp/twitter-roberta-base-sentiment with Hugging Face Trainer ---\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d175c2a927484622be048500df0f456d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/9878 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "78665c31cc5b4a369712624adcd4a6cb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/8232 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment and are newly initialized because the shapes did not match:\n",
            "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
            "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/data/tmp/ipykernel_3475/3955368469.py:50: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/data/wandb/wandb/run-20250809_112834-5691reqi</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/5691reqi' target=\"_blank\">hf-trainer-twitter-roberta-base-sentiment</a></strong> to <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/5691reqi' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/5691reqi</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1545' max='1545' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1545/1545 11:23, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.890800</td>\n",
              "      <td>0.839120</td>\n",
              "      <td>0.669961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.637500</td>\n",
              "      <td>0.729731</td>\n",
              "      <td>0.733533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.394000</td>\n",
              "      <td>0.746678</td>\n",
              "      <td>0.746080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.280600</td>\n",
              "      <td>0.855474</td>\n",
              "      <td>0.753194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.175600</td>\n",
              "      <td>0.944261</td>\n",
              "      <td>0.760618</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./results/twitter-roberta-base-sentiment/checkpoint-1545)... "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving best model from trainer found at: ./results/twitter-roberta-base-sentiment/checkpoint-1545\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Done. 7.7s\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>▁▆▇▇█</td></tr><tr><td>eval/loss</td><td>▅▁▂▅█</td></tr><tr><td>eval/runtime</td><td>▃▇▁▇█</td></tr><tr><td>eval/samples_per_second</td><td>▆▂█▂▁</td></tr><tr><td>eval/steps_per_second</td><td>▆▂█▂▁</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▃▃▄▃▂▆▆▄▅▁█▂▂▅▄</td></tr><tr><td>train/learning_rate</td><td>█▇▇▆▆▅▅▄▄▃▃▃▂▁▁</td></tr><tr><td>train/loss</td><td>█▆▆▄▄▄▃▃▂▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.76062</td></tr><tr><td>eval/loss</td><td>0.94426</td></tr><tr><td>eval/runtime</td><td>29.0734</td></tr><tr><td>eval/samples_per_second</td><td>283.145</td></tr><tr><td>eval/steps_per_second</td><td>4.437</td></tr><tr><td>total_flos</td><td>1.299540505629696e+16</td></tr><tr><td>train/epoch</td><td>5</td></tr><tr><td>train/global_step</td><td>1545</td></tr><tr><td>train/grad_norm</td><td>11.07288</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.1756</td></tr><tr><td>train_loss</td><td>0.50791</td></tr><tr><td>train_runtime</td><td>685.1944</td></tr><tr><td>train_samples_per_second</td><td>72.082</td></tr><tr><td>train_steps_per_second</td><td>2.255</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">hf-trainer-twitter-roberta-base-sentiment</strong> at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/5691reqi' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/5691reqi</a><br> View project at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a><br>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>/data/wandb/wandb/run-20250809_112834-5691reqi/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Training roberta-base with Hugging Face Trainer ---\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "53d4dcda01914ecab0f1e201bd455873",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/9878 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e391a409c5cf42708650407e91bb6341",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/8232 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/data/tmp/ipykernel_3475/3955368469.py:50: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/data/wandb/wandb/run-20250809_114024-w5yinwi8</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/w5yinwi8' target=\"_blank\">hf-trainer-roberta-base</a></strong> to <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/w5yinwi8' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/w5yinwi8</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1545' max='1545' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1545/1545 11:23, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.005000</td>\n",
              "      <td>0.975701</td>\n",
              "      <td>0.609225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.751800</td>\n",
              "      <td>0.749547</td>\n",
              "      <td>0.718719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.500400</td>\n",
              "      <td>0.791112</td>\n",
              "      <td>0.709633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.398500</td>\n",
              "      <td>0.743602</td>\n",
              "      <td>0.749820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.257400</td>\n",
              "      <td>0.856273</td>\n",
              "      <td>0.754155</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./results/roberta-base/checkpoint-1545)... "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving best model from trainer found at: ./results/roberta-base/checkpoint-1545\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Done. 7.7s\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>▁▆▆██</td></tr><tr><td>eval/loss</td><td>█▁▂▁▄</td></tr><tr><td>eval/runtime</td><td>▁▄█▂▂</td></tr><tr><td>eval/samples_per_second</td><td>█▅▁▇▇</td></tr><tr><td>eval/steps_per_second</td><td>█▅▁▇▇</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▆▄▄▇▃▆▅▄▂█▇▅▇▁▄</td></tr><tr><td>train/learning_rate</td><td>█▇▇▆▆▅▅▄▄▃▃▃▂▁▁</td></tr><tr><td>train/loss</td><td>█▆▅▄▄▄▃▃▂▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.75415</td></tr><tr><td>eval/loss</td><td>0.85627</td></tr><tr><td>eval/runtime</td><td>28.878</td></tr><tr><td>eval/samples_per_second</td><td>285.061</td></tr><tr><td>eval/steps_per_second</td><td>4.467</td></tr><tr><td>total_flos</td><td>1.299540505629696e+16</td></tr><tr><td>train/epoch</td><td>5</td></tr><tr><td>train/global_step</td><td>1545</td></tr><tr><td>train/grad_norm</td><td>11.77544</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.2574</td></tr><tr><td>train_loss</td><td>0.62717</td></tr><tr><td>train_runtime</td><td>684.0265</td></tr><tr><td>train_samples_per_second</td><td>72.205</td></tr><tr><td>train_steps_per_second</td><td>2.259</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">hf-trainer-roberta-base</strong> at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/w5yinwi8' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/w5yinwi8</a><br> View project at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a><br>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>/data/wandb/wandb/run-20250809_114024-w5yinwi8/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def compute_metrics_for_trainer(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return {'f1': f1_score(labels, predictions, average='weighted')}\n",
        "\n",
        "# הגדרת המודלים לאימון\n",
        "models_to_train_with_hf = [\"cardiffnlp/twitter-roberta-base-sentiment\", \"roberta-base\"]\n",
        "\n",
        "# הכנת הדאטהסטים\n",
        "sentiment_mapping = {'Extremely Negative': 0, 'Negative': 1, 'Neutral': 2, 'Positive': 3, 'Extremely Positive': 4}\n",
        "train_temp_df = train_df[['clean_text', 'Sentiment']].rename(columns={'Sentiment': 'label'})\n",
        "val_temp_df = val_df[['clean_text', 'Sentiment']].rename(columns={'Sentiment': 'label'})\n",
        "train_temp_df['label'] = train_temp_df['label'].map(sentiment_mapping)\n",
        "val_temp_df['label'] = val_temp_df['label'].map(sentiment_mapping)\n",
        "train_dataset_hf = HFDataset.from_pandas(train_temp_df.sample(frac=0.3, random_state=42))\n",
        "val_dataset_hf = HFDataset.from_pandas(val_temp_df)\n",
        "\n",
        "# לולאה על המודלים\n",
        "for model_name in models_to_train_with_hf:\n",
        "    print(f\"\\n--- Training {model_name} with Hugging Face Trainer ---\")\n",
        "    model_short_name = model_name.split('/')[-1]\n",
        "    \n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "    \n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(examples['clean_text'], truncation=True, padding='max_length', max_length=512)\n",
        "        \n",
        "    train_tokenized = train_dataset_hf.map(tokenize_function, batched=True)\n",
        "    val_tokenized = val_dataset_hf.map(tokenize_function, batched=True)\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5, ignore_mismatched_sizes=True)\n",
        "    \n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f'./results/{model_short_name}',\n",
        "        num_train_epochs=5,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=32,\n",
        "        logging_dir=f'./logs/{model_short_name}',\n",
        "        logging_steps=100,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1\",\n",
        "        greater_is_better=True,\n",
        "        report_to=\"wandb\",\n",
        "        run_name=f\"hf-trainer-{model_short_name}\",\n",
        "        save_total_limit=1,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model, args=training_args, train_dataset=train_tokenized,\n",
        "        eval_dataset=val_tokenized, tokenizer=tokenizer, compute_metrics=compute_metrics_for_trainer\n",
        "    )\n",
        "    \n",
        "    run = wandb.init(project=\"moti-matan-tel-aviv-university\", name=f\"hf-trainer-{model_short_name}\", reinit=True)\n",
        "    trainer.train()\n",
        "    \n",
        "    best_model_path = trainer.state.best_model_checkpoint\n",
        "    if best_model_path:\n",
        "        print(f\"Saving best model from trainer found at: {best_model_path}\")\n",
        "        artifact = wandb.Artifact(\n",
        "            name=f\"trainer-{model_short_name}\",\n",
        "            type='model',\n",
        "            # --- הוספת תיאור ברור ---\n",
        "            description=f\"Best model for {model_name} trained using the Hugging Face Trainer.\"\n",
        "        )\n",
        "        artifact.add_dir(best_model_path)\n",
        "        run.log_artifact(artifact, aliases=['best'])\n",
        "    \n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "GcO_LGLCXpOR"
      },
      "outputs": [],
      "source": [
        "def evaluate_on_test_set(model, test_loader, model_name=\"model\"):\n",
        "    \"\"\"\n",
        "    Evaluates a model's performance on the test set, returning a comprehensive\n",
        "    set of metrics (Accuracy, F1, Precision, Recall).\n",
        "    \"\"\"\n",
        "    model.to(device).eval() # ודא שהמודל במצב הערכה ועל המכשיר הנכון\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    # אין צורך בחישוב גרדיאנטים בשלב ההערכה\n",
        "    with torch.no_grad():\n",
        "        # עטיפת ה-loader עם tqdm להצגת פס התקדמות\n",
        "        for batch in tqdm(test_loader, desc=f\"Evaluating {model_name} on Test Set\"):\n",
        "            # העברת המנה (batch) לאותו מכשיר שבו נמצא המודל\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'] # שמירת התוויות לחישוב המדדים\n",
        "\n",
        "            # קבלת תחזיות המודל\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            predictions = outputs.logits.argmax(dim=1)\n",
        "            \n",
        "            # איסוף התוויות והתחזיות מכל המנות\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(predictions.cpu().numpy())\n",
        "    \n",
        "    # חישוב כל המדדים הרלוונטיים\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    \n",
        "    # החזרת המדדים במילון\n",
        "    return {\n",
        "        \"Accuracy\": accuracy,\n",
        "        \"F1 Score\": f1,\n",
        "        \"Precision\": precision,\n",
        "        \"Recall\": recall\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compression helpers\n",
        "def compress_prune_model(model, prune_percent=0.4):\n",
        "    model_to_prune = deepcopy(model).to('cpu')\n",
        "    parameters_to_prune = [(module, 'weight') for module in model_to_prune.modules() if isinstance(module, nn.Linear)]\n",
        "    if parameters_to_prune:\n",
        "        prune.global_unstructured(parameters_to_prune, pruning_method=prune.L1Unstructured, amount=prune_percent)\n",
        "    return model_to_prune\n",
        "\n",
        "def compress_quantize_model(model):\n",
        "    return quantize_dynamic(deepcopy(model).to('cpu'), {nn.Linear}, dtype=torch.qint8)\n",
        "\n",
        "def evaluate_compressed_model(model, test_loader, model_name=\"model\"):\n",
        "    \"\"\"Evaluates a model's performance, size, and inference time.\"\"\"\n",
        "    eval_device = 'cpu' if 'quantized' in model_name.lower() else device\n",
        "    model.to(eval_device).eval()\n",
        "\n",
        "    all_labels, all_preds, inference_times = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc=f\"Evaluating {model_name}\", leave=False):\n",
        "            input_ids = batch['input_ids'].to(eval_device)\n",
        "            attention_mask = batch['attention_mask'].to(eval_device)\n",
        "\n",
        "            start_time = time.time()\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            inference_times.append(time.time() - start_time)\n",
        "\n",
        "            all_labels.extend(batch['labels'].numpy())\n",
        "            all_preds.extend(outputs.logits.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "    size_mb = sum(p.element_size() * p.numel() for p in model.parameters()) / (1024 * 1024)\n",
        "\n",
        "    return {\n",
        "        \"F1 Score\": f1_score(all_labels, all_preds, average='weighted'),\n",
        "        \"Accuracy\": accuracy_score(all_labels, all_preds),\n",
        "        \"Avg Inference Time (ms)\": np.mean(inference_times) * 1000,\n",
        "        \"Model Size (MB)\": size_mb\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_best_model(model_name, method):\n",
        "    \"\"\"Downloads the best model artifact from W&B for a given method.\"\"\"\n",
        "    print(f\"\\n--- Loading best model for {model_name} (Method: {method}) ---\")\n",
        "    model_short_name = model_name.split('/')[-1]\n",
        "    artifact_name = f\"{method}-{model_short_name}:best\"\n",
        "    \n",
        "    run = wandb.init(project=\"moti-matan-tel-aviv-university\", job_type='evaluation', reinit=True)\n",
        "    try:\n",
        "        artifact = run.use_artifact(artifact_name)\n",
        "        artifact_dir = artifact.download()\n",
        "        \n",
        "        if os.path.exists(os.path.join(artifact_dir, \"config.json\")):\n",
        "            model = AutoModelForSequenceClassification.from_pretrained(artifact_dir, num_labels=5)\n",
        "        else:\n",
        "            model_path = os.path.join(artifact_dir, \"model.pt\")\n",
        "            model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5, ignore_mismatched_sizes=True)\n",
        "            model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "        return model\n",
        "    finally:\n",
        "        run.finish()\n",
        "\n",
        "def evaluate_on_test_set(model, test_loader, model_name=\"model\"):\n",
        "    \"\"\"Evaluates a model's performance on the test set.\"\"\"\n",
        "    model.to(device).eval()\n",
        "    all_labels, all_preds = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc=f\"Evaluating {model_name} on Test Set\"):\n",
        "            outputs = model(\n",
        "                input_ids=batch['input_ids'].to(device),\n",
        "                attention_mask=batch['attention_mask'].to(device)\n",
        "            )\n",
        "            all_labels.extend(batch['labels'].numpy())\n",
        "            all_preds.extend(outputs.logits.argmax(dim=1).cpu().numpy())\n",
        "    return {\n",
        "        \"Accuracy\": accuracy_score(all_labels, all_preds),\n",
        "        \"F1 Score\": f1_score(all_labels, all_preds, average='weighted')\n",
        "    }\n",
        "\n",
        "def compress_prune_model(model, prune_percent=0.4):\n",
        "    model_to_prune = deepcopy(model).to('cpu')\n",
        "    parameters_to_prune = [(module, 'weight') for module in model_to_prune.modules() if isinstance(module, nn.Linear)]\n",
        "    if parameters_to_prune:\n",
        "        prune.global_unstructured(parameters_to_prune, pruning_method=prune.L1Unstructured, amount=prune_percent)\n",
        "    return model_to_prune\n",
        "\n",
        "def compress_quantize_model(model):\n",
        "    return quantize_dynamic(deepcopy(model).to('cpu'), {nn.Linear}, dtype=torch.qint8)\n",
        "\n",
        "def evaluate_compressed_model(model, test_loader, model_name=\"model\"):\n",
        "    \"\"\"Evaluates a model's performance, size, and inference time.\"\"\"\n",
        "    eval_device = 'cpu' if 'Quantized' in model_name else device\n",
        "    model.to(eval_device).eval()\n",
        "    \n",
        "    all_labels, all_preds, inference_times = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc=f\"Evaluating {model_name}\", leave=False):\n",
        "            input_ids = batch['input_ids'].to(eval_device)\n",
        "            attention_mask = batch['attention_mask'].to(eval_device)\n",
        "            \n",
        "            start_time = time.time()\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            inference_times.append(time.time() - start_time)\n",
        "            \n",
        "            all_labels.extend(batch['labels'].cpu().numpy())\n",
        "            all_preds.extend(outputs.logits.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "    temp_model = deepcopy(model)\n",
        "    for module in temp_model.modules():\n",
        "        if isinstance(module, torch.nn.Linear) and prune.is_pruned(module):\n",
        "            prune.remove(module, 'weight')\n",
        "    size_mb = sum(p.element_size() * p.numel() for p in temp_model.parameters()) / (1024 * 1024)\n",
        "    del temp_model\n",
        "\n",
        "    return {\n",
        "        \"F1 Score\": f1_score(all_labels, all_preds, average='weighted'),\n",
        "        \"Accuracy\": accuracy_score(all_labels, all_preds),\n",
        "        \"Avg Inference Time (ms)\": np.mean(inference_times) * 1000,\n",
        "        \"Model Size (MB)\": size_mb\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "uJK_JICNXpQx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Loading best model for cardiffnlp/twitter-roberta-base-sentiment (Method: manual) ---\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/data/wandb/wandb/run-20250809_115214-rl49hmv6</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/rl49hmv6' target=\"_blank\">breezy-snowflake-238</a></strong> to <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/rl49hmv6' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/rl49hmv6</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact manual-twitter-roberta-base-sentiment:best, 475.58MB. 1 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
            "Done. 0:0:7.3 (65.1MB/s)\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment and are newly initialized because the shapes did not match:\n",
            "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
            "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">breezy-snowflake-238</strong> at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/rl49hmv6' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/rl49hmv6</a><br> View project at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>/data/wandb/wandb/run-20250809_115214-rl49hmv6/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dab1aa6b532946d483d798c9f5631d33",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating cardiffnlp/twitter-roberta-base-sentiment (manual) on Test Set:   0%|          | 0/119 [00:00<?, ?i…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Loading best model for cardiffnlp/twitter-roberta-base-sentiment (Method: trainer) ---\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/data/wandb/wandb/run-20250809_115246-o3lrxbi9</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/o3lrxbi9' target=\"_blank\">giddy-donkey-239</a></strong> to <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/o3lrxbi9' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/o3lrxbi9</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact trainer-twitter-roberta-base-sentiment:best, 1431.30MB. 12 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   12 of 12 files downloaded.  \n",
            "Done. 0:0:1.0 (1479.0MB/s)\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">giddy-donkey-239</strong> at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/o3lrxbi9' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/o3lrxbi9</a><br> View project at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>/data/wandb/wandb/run-20250809_115246-o3lrxbi9/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb9e7e450e7a4545b583034bc8589a2f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating cardiffnlp/twitter-roberta-base-sentiment (trainer) on Test Set:   0%|          | 0/119 [00:00<?, ?…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/data/wandb/wandb/run-20250809_115312-mao7c9b8</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/mao7c9b8' target=\"_blank\">legendary-firebrand-240</a></strong> to <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/mao7c9b8' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/mao7c9b8</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🏆 Trainer is the champion for twitter-roberta-base-sentiment!\n",
            "Successfully added alias 'champion-twitter-roberta-base-sentiment' to trainer-twitter-roberta-base-sentiment:best\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">legendary-firebrand-240</strong> at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/mao7c9b8' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/mao7c9b8</a><br> View project at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>/data/wandb/wandb/run-20250809_115312-mao7c9b8/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Loading best model for roberta-base (Method: manual) ---\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/data/wandb/wandb/run-20250809_115314-7f5hqabj</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/7f5hqabj' target=\"_blank\">breezy-rain-241</a></strong> to <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/7f5hqabj' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/7f5hqabj</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact manual-roberta-base:best, 475.58MB. 1 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
            "Done. 0:0:8.6 (55.3MB/s)\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">breezy-rain-241</strong> at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/7f5hqabj' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/7f5hqabj</a><br> View project at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>/data/wandb/wandb/run-20250809_115314-7f5hqabj/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "120f6b1125ac450982481d848a8ba9fe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating roberta-base (manual) on Test Set:   0%|          | 0/119 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Loading best model for roberta-base (Method: trainer) ---\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/data/wandb/wandb/run-20250809_115351-3ozteygc</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/3ozteygc' target=\"_blank\">fine-morning-242</a></strong> to <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/3ozteygc' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/3ozteygc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact trainer-roberta-base:best, 1431.30MB. 12 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   12 of 12 files downloaded.  \n",
            "Done. 0:0:0.9 (1590.2MB/s)\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fine-morning-242</strong> at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/3ozteygc' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/3ozteygc</a><br> View project at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>/data/wandb/wandb/run-20250809_115351-3ozteygc/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1345cc8dc0044d548c5e1857a0d43768",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating roberta-base (trainer) on Test Set:   0%|          | 0/119 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/data/wandb/wandb/run-20250809_115416-i1oqxjoh</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/i1oqxjoh' target=\"_blank\">super-durian-243</a></strong> to <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/i1oqxjoh' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/i1oqxjoh</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🏆 Trainer is the champion for roberta-base!\n",
            "Successfully added alias 'champion-roberta-base' to trainer-roberta-base:best\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">super-durian-243</strong> at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/i1oqxjoh' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/i1oqxjoh</a><br> View project at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>/data/wandb/wandb/run-20250809_115416-i1oqxjoh/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- CHAMPIONSHIP: Final Test Results ---\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1 Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>cardiffnlp/twitter-roberta-base-sentiment (manual)</th>\n",
              "      <td>0.6172</td>\n",
              "      <td>0.6160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cardiffnlp/twitter-roberta-base-sentiment (trainer)</th>\n",
              "      <td>0.7378</td>\n",
              "      <td>0.7388</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-base (manual)</th>\n",
              "      <td>0.5948</td>\n",
              "      <td>0.5990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-base (trainer)</th>\n",
              "      <td>0.7388</td>\n",
              "      <td>0.7397</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    Accuracy  F1 Score\n",
              "cardiffnlp/twitter-roberta-base-sentiment (manual)    0.6172    0.6160\n",
              "cardiffnlp/twitter-roberta-base-sentiment (trai...    0.7378    0.7388\n",
              "roberta-base (manual)                                 0.5948    0.5990\n",
              "roberta-base (trainer)                                0.7388    0.7397"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# --- FINAL COMPETITION ---\n",
        "all_test_results = {}\n",
        "champion_models = {} # Dictionary to hold the two winning models\n",
        "models_to_evaluate = [\"cardiffnlp/twitter-roberta-base-sentiment\", \"roberta-base\"]\n",
        "tokenizer = AutoTokenizer.from_pretrained(models_to_evaluate[0])\n",
        "test_dataset = TweetsDataset(test_df, tokenizer)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# --- Evaluate all 4 models and find the champion for each base model ---\n",
        "for model_name in models_to_evaluate:\n",
        "    manual_model = get_best_model(model_name, \"manual\")\n",
        "    manual_results = evaluate_on_test_set(manual_model, test_loader, f\"{model_name} (manual)\")\n",
        "    all_test_results[f\"{model_name} (manual)\"] = manual_results\n",
        "\n",
        "    trainer_model = get_best_model(model_name, \"trainer\")\n",
        "    trainer_results = evaluate_on_test_set(trainer_model, test_loader, f\"{model_name} (trainer)\")\n",
        "    all_test_results[f\"{model_name} (trainer)\"] = trainer_results\n",
        "    \n",
        "    # --- Tag the champion for this model type in W&B ---\n",
        "    run = wandb.init(project=\"moti-matan-tel-aviv-university\", job_type='tagging', reinit=True)\n",
        "    model_short_name = model_name.split('/')[-1]\n",
        "    champion_alias = f\"champion-{model_short_name}\"\n",
        "    \n",
        "    if trainer_results[\"F1 Score\"] > manual_results[\"F1 Score\"]:\n",
        "        print(f\"🏆 Trainer is the champion for {model_short_name}!\")\n",
        "        artifact_to_tag = f\"trainer-{model_short_name}:best\"\n",
        "        champion_models[model_name] = trainer_model\n",
        "    else:\n",
        "        print(f\"🏆 Manual/Optuna is the champion for {model_short_name}!\")\n",
        "        artifact_to_tag = f\"manual-{model_short_name}:best\"\n",
        "        champion_models[model_name] = manual_model\n",
        "            \n",
        "    try:\n",
        "        artifact = run.use_artifact(artifact_to_tag)\n",
        "        if champion_alias not in artifact.aliases:\n",
        "            artifact.aliases.append(champion_alias)\n",
        "            artifact.save()\n",
        "            print(f\"Successfully added alias '{champion_alias}' to {artifact_to_tag}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not apply champion alias: {e}\")\n",
        "    finally:\n",
        "        run.finish()\n",
        "\n",
        "print(\"\\n--- CHAMPIONSHIP: Final Test Results ---\")\n",
        "results_df = pd.DataFrame.from_dict(all_test_results, orient='index')\n",
        "display(results_df.round(4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "LQw9rtAtXvTA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================== Compression Analysis for Champion: cardiffnlp/twitter-roberta-base-sentiment ====================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c8ec959e63941149d48ad75befbb6e7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating Original:   0%|          | 0/119 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "897e6a0c1c8d4695971f0466aa38d127",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating Pruned:   0%|          | 0/119 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "50bc29aadcf34af789893e0331810412",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating Quantized:   0%|          | 0/119 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f64eb335c52c4f3d93f72792a2788cc9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d3ba6ec1c0a04418a1e02e6f58c2e072",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/331M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55a798768cc4451b89bbabee23e7500b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating Distilled:   0%|          | 0/119 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>F1 Score</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Avg Inference Time (ms)</th>\n",
              "      <th>Model Size (MB)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Original</th>\n",
              "      <td>0.7388</td>\n",
              "      <td>0.7378</td>\n",
              "      <td>6.2664</td>\n",
              "      <td>475.5000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Pruned (40%)</th>\n",
              "      <td>0.4524</td>\n",
              "      <td>0.5269</td>\n",
              "      <td>8.5305</td>\n",
              "      <td>475.5000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Quantized (INT8)</th>\n",
              "      <td>0.7246</td>\n",
              "      <td>0.7246</td>\n",
              "      <td>1388.7191</td>\n",
              "      <td>148.9160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Distilled (Untrained)</th>\n",
              "      <td>0.1181</td>\n",
              "      <td>0.2730</td>\n",
              "      <td>3.8305</td>\n",
              "      <td>313.2715</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       F1 Score  Accuracy  Avg Inference Time (ms)  \\\n",
              "Original                 0.7388    0.7378                   6.2664   \n",
              "Pruned (40%)             0.4524    0.5269                   8.5305   \n",
              "Quantized (INT8)         0.7246    0.7246                1388.7191   \n",
              "Distilled (Untrained)    0.1181    0.2730                   3.8305   \n",
              "\n",
              "                       Model Size (MB)  \n",
              "Original                      475.5000  \n",
              "Pruned (40%)                  475.5000  \n",
              "Quantized (INT8)              148.9160  \n",
              "Distilled (Untrained)         313.2715  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================== Compression Analysis for Champion: roberta-base ====================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "02cf270e9fdc4bdea944e04399142b53",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating Original:   0%|          | 0/119 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a893410c533e46b19d22f8078d9e18dd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating Pruned:   0%|          | 0/119 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7d65aaeb7c1443418b78c289495430c1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating Quantized:   0%|          | 0/119 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a90fd2894ad24e3fb1cb25cd17a34c6d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating Distilled:   0%|          | 0/119 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/nlp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>F1 Score</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Avg Inference Time (ms)</th>\n",
              "      <th>Model Size (MB)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Original</th>\n",
              "      <td>0.7397</td>\n",
              "      <td>0.7388</td>\n",
              "      <td>6.2824</td>\n",
              "      <td>475.5000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Pruned (40%)</th>\n",
              "      <td>0.3652</td>\n",
              "      <td>0.4476</td>\n",
              "      <td>8.6335</td>\n",
              "      <td>475.5000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Quantized (INT8)</th>\n",
              "      <td>0.7157</td>\n",
              "      <td>0.7146</td>\n",
              "      <td>1391.2147</td>\n",
              "      <td>148.9160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Distilled (Untrained)</th>\n",
              "      <td>0.1194</td>\n",
              "      <td>0.2738</td>\n",
              "      <td>3.7978</td>\n",
              "      <td>313.2715</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       F1 Score  Accuracy  Avg Inference Time (ms)  \\\n",
              "Original                 0.7397    0.7388                   6.2824   \n",
              "Pruned (40%)             0.3652    0.4476                   8.6335   \n",
              "Quantized (INT8)         0.7157    0.7146                1391.2147   \n",
              "Distilled (Untrained)    0.1194    0.2738                   3.7978   \n",
              "\n",
              "                       Model Size (MB)  \n",
              "Original                      475.5000  \n",
              "Pruned (40%)                  475.5000  \n",
              "Quantized (INT8)              148.9160  \n",
              "Distilled (Untrained)         313.2715  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# --- Run Compression Analysis on EACH of the two champions ---\n",
        "for model_name, winning_model in champion_models.items():\n",
        "    print(f\"\\n{'='*20} Compression Analysis for Champion: {model_name} {'='*20}\")\n",
        "    compression_results = {}\n",
        "    \n",
        "    # 1. Original Model (The Champion)\n",
        "    compression_results['Original'] = evaluate_compressed_model(winning_model, test_loader, \"Original\")\n",
        "\n",
        "    # 2. Pruned Model\n",
        "    pruned_model = compress_prune_model(winning_model)\n",
        "    compression_results['Pruned (40%)'] = evaluate_compressed_model(pruned_model, test_loader, \"Pruned\")\n",
        "\n",
        "    # 3. Quantized Model\n",
        "    quantized_model = compress_quantize_model(winning_model)\n",
        "    compression_results['Quantized (INT8)'] = evaluate_compressed_model(quantized_model, test_loader, \"Quantized\")\n",
        "    \n",
        "    # 4. Distilled Model (simulated with a smaller architecture)\n",
        "    distilled_model = AutoModelForSequenceClassification.from_pretrained('distilroberta-base', num_labels=5)\n",
        "    compression_results['Distilled (Untrained)'] = evaluate_compressed_model(distilled_model, test_loader, \"Distilled\")\n",
        "\n",
        "    compression_df = pd.DataFrame.from_dict(compression_results, orient='index')\n",
        "    display(compression_df.round(4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xGJKiYrXw9r"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

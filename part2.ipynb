{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8p_PNOfucPYZ",
   "metadata": {
    "id": "8p_PNOfucPYZ"
   },
   "source": [
    "# Part 2- Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f8c0d1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'optuna'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcopy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deepcopy\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpruners\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MedianPruner\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'optuna'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "import optuna\n",
    "import wandb\n",
    "from optuna.pruners import MedianPruner\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch import nn, optim\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from torch.utils.data import Subset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe96d20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851e7c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.amp.autocast('cuda')   # FP16\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ec75d4",
   "metadata": {},
   "source": [
    "Dataset creation and formatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc151111",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = kagglehub.dataset_download(\"datatattle/covid-19-nlp-text-classification\")\n",
    "print(\"Path to dataset files:\", data_path)\n",
    "train_df = pd.read_csv(os.path.join(data_path, \"Corona_NLP_train.csv\"), encoding=\"latin1\")\n",
    "test_df = pd.read_csv(os.path.join(data_path, \"Corona_NLP_test.csv\"), encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89a3924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3799</td>\n",
       "      <td>48751</td>\n",
       "      <td>London</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv and and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3800</td>\n",
       "      <td>48752</td>\n",
       "      <td>UK</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3801</td>\n",
       "      <td>48753</td>\n",
       "      <td>Vagabonds</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3802</td>\n",
       "      <td>48754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3803</td>\n",
       "      <td>48755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserName  ScreenName   Location     TweetAt  \\\n",
       "0      3799       48751     London  16-03-2020   \n",
       "1      3800       48752         UK  16-03-2020   \n",
       "2      3801       48753  Vagabonds  16-03-2020   \n",
       "3      3802       48754        NaN  16-03-2020   \n",
       "4      3803       48755        NaN  16-03-2020   \n",
       "\n",
       "                                       OriginalTweet           Sentiment  \\\n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral   \n",
       "1  advice Talk to your neighbours family to excha...            Positive   \n",
       "2  Coronavirus Australia: Woolworths to give elde...            Positive   \n",
       "3  My food stock is not the only one which is emp...            Positive   \n",
       "4  Me, ready to go at supermarket during the #COV...  Extremely Negative   \n",
       "\n",
       "                                          clean_text  \n",
       "0            @MeNyrbie @Phil_Gahan @Chrisitv and and  \n",
       "1  advice Talk to your neighbours family to excha...  \n",
       "2  Coronavirus Australia: Woolworths to give elde...  \n",
       "3  My food stock is not the only one which is emp...  \n",
       "4  Me, ready to go at supermarket during the #COV...  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def delete_words_with_http(text, to_remove=\"http\"):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    words = text.split()\n",
    "    kept_words = [word for word in words if to_remove not in word]\n",
    "    return ' '.join(kept_words)\n",
    "\n",
    "train_df['clean_text'] = train_df['OriginalTweet'].apply(lambda x: delete_words_with_http(x))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TLD-oxjfqmAD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "TLD-oxjfqmAD",
    "outputId": "2a97f552-d2a1-4fc3-eed0-75bad179e3c0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>44953</td>\n",
       "      <td>NYC</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>44954</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>44955</td>\n",
       "      <td>NaN</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>Find out how you can protect yourself and love...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "      <td>Find out how you can protect yourself and love...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>44956</td>\n",
       "      <td>Chicagoland</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>44957</td>\n",
       "      <td>Melbourne, Victoria</td>\n",
       "      <td>03-03-2020</td>\n",
       "      <td>#toiletpaper #dunnypaper #coronavirus #coronav...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>#toiletpaper #dunnypaper #coronavirus #coronav...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserName  ScreenName             Location     TweetAt  \\\n",
       "0         1       44953                  NYC  02-03-2020   \n",
       "1         2       44954          Seattle, WA  02-03-2020   \n",
       "2         3       44955                  NaN  02-03-2020   \n",
       "3         4       44956          Chicagoland  02-03-2020   \n",
       "4         5       44957  Melbourne, Victoria  03-03-2020   \n",
       "\n",
       "                                       OriginalTweet           Sentiment  \\\n",
       "0  TRENDING: New Yorkers encounter empty supermar...  Extremely Negative   \n",
       "1  When I couldn't find hand sanitizer at Fred Me...            Positive   \n",
       "2  Find out how you can protect yourself and love...  Extremely Positive   \n",
       "3  #Panic buying hits #NewYork City as anxious sh...            Negative   \n",
       "4  #toiletpaper #dunnypaper #coronavirus #coronav...             Neutral   \n",
       "\n",
       "                                          clean_text  \n",
       "0  TRENDING: New Yorkers encounter empty supermar...  \n",
       "1  When I couldn't find hand sanitizer at Fred Me...  \n",
       "2  Find out how you can protect yourself and love...  \n",
       "3  #Panic buying hits #NewYork City as anxious sh...  \n",
       "4  #toiletpaper #dunnypaper #coronavirus #coronav...  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['clean_text']= test_df['OriginalTweet'].apply(lambda x: delete_words_with_http(x))\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "53295fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41157, 7)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60f2e80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31089</th>\n",
       "      <td>34888</td>\n",
       "      <td>79840</td>\n",
       "      <td>Lagos</td>\n",
       "      <td>06-04-2020</td>\n",
       "      <td>Without the there would not be any problem wh...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Without the there would not be any problem wha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35564</th>\n",
       "      <td>39363</td>\n",
       "      <td>84315</td>\n",
       "      <td>NaN</td>\n",
       "      <td>09-04-2020</td>\n",
       "      <td>Rice &amp;amp; wheat prices surge amid fears Covid...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>Rice &amp;amp; wheat prices surge amid fears Covid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>3943</td>\n",
       "      <td>48895</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>When the government says to start social dista...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>When the government says to start social dista...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8202</th>\n",
       "      <td>12001</td>\n",
       "      <td>56953</td>\n",
       "      <td>irlande du nord</td>\n",
       "      <td>19-03-2020</td>\n",
       "      <td>What the shops are doing is obeying the law of...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>What the shops are doing is obeying the law of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31720</th>\n",
       "      <td>35519</td>\n",
       "      <td>80471</td>\n",
       "      <td>Zaria, Nigeria</td>\n",
       "      <td>07-04-2020</td>\n",
       "      <td>Kaduna State Task Force on Covid 19 led by the...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Kaduna State Task Force on Covid 19 led by the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       UserName  ScreenName         Location     TweetAt  \\\n",
       "31089     34888       79840            Lagos  06-04-2020   \n",
       "35564     39363       84315              NaN  09-04-2020   \n",
       "144        3943       48895              NaN  16-03-2020   \n",
       "8202      12001       56953  irlande du nord  19-03-2020   \n",
       "31720     35519       80471   Zaria, Nigeria  07-04-2020   \n",
       "\n",
       "                                           OriginalTweet           Sentiment  \\\n",
       "31089   Without the there would not be any problem wh...             Neutral   \n",
       "35564  Rice &amp; wheat prices surge amid fears Covid...  Extremely Negative   \n",
       "144    When the government says to start social dista...            Positive   \n",
       "8202   What the shops are doing is obeying the law of...            Positive   \n",
       "31720  Kaduna State Task Force on Covid 19 led by the...            Negative   \n",
       "\n",
       "                                              clean_text  \n",
       "31089  Without the there would not be any problem wha...  \n",
       "35564  Rice &amp; wheat prices surge amid fears Covid...  \n",
       "144    When the government says to start social dista...  \n",
       "8202   What the shops are doing is obeying the law of...  \n",
       "31720  Kaduna State Task Force on Covid 19 led by the...  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c2169e",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7vWdB_jUcT1K",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7vWdB_jUcT1K",
    "outputId": "a7344061-b5bb-4276-9f95-3cf3ecb1eb2e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base-sentiment and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, RobertaForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\", use_fast=False)\n",
    "model1 = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\", num_labels=5, ignore_mismatched_sizes=True).to(device)\n",
    "model2 = AutoModelForSequenceClassification.from_pretrained('cardiffnlp/twitter-xlm-roberta-base-sentiment', num_labels=5, ignore_mismatched_sizes=True).to(device) # initialize RoBerta large from HF, num_labels=2 -> 2 classes.\n",
    "models=[model1, model2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "YTg_eLigp99W",
   "metadata": {
    "id": "YTg_eLigp99W"
   },
   "outputs": [],
   "source": [
    "class Tweets(Dataset): # Dataset Class\n",
    "    def __init__(self, dataframe, tokenizer):\n",
    "        # Map sentiment strings to numerical labels\n",
    "        sentiment_mapping = {\n",
    "            'Extremely Negative': 0,\n",
    "            'Negative': 1,\n",
    "            'Neutral': 2,\n",
    "            'Positive': 3,\n",
    "            'Extremely Positive': 4\n",
    "        }\n",
    "        self.texts = dataframe['clean_text'].tolist()\n",
    "        self.labels = dataframe['Sentiment'].map(sentiment_mapping).tolist() # Map sentiment strings to numerical labels\n",
    "        self.tokenizer = tokenizer # Tokenizer for text processing\n",
    "\n",
    "    def __len__(self): #Returns the total number of samples in the dataset.\n",
    "        # This method is required for PyTorch's DataLoader to work !!\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx): #Retrieves a single data sample and its label at the specified index.\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        label = torch.tensor(label, dtype=torch.long) \n",
    "        # Tokenize the text using the provided tokenizer\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length', # Add padding to reach the maximum length\n",
    "            truncation=True, # Trim if the text is longer than max_length\n",
    "            max_length=512, # Maximum sequence length allowed\n",
    "            return_tensors='pt')\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': label.detach().clone() if isinstance(label, torch.Tensor) else torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "Yit27A0vrFRJ",
   "metadata": {
    "id": "Yit27A0vrFRJ"
   },
   "outputs": [],
   "source": [
    "def early_stop_check(patience, best_val_accuracy, best_val_accuracy_epoch, current_val_accuracy, current_val_accuracy_epoch):\n",
    "    early_stop_flag = False\n",
    "    if current_val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = current_val_accuracy\n",
    "        best_val_accuracy_epoch = current_val_accuracy_epoch\n",
    "    else:\n",
    "        if current_val_accuracy_epoch - best_val_accuracy_epoch > patience:\n",
    "            early_stop_flag = True\n",
    "    return best_val_accuracy, best_val_accuracy_epoch, early_stop_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JqAgWBojrHC4",
   "metadata": {
    "id": "JqAgWBojrHC4"
   },
   "outputs": [],
   "source": [
    "def train_model_with_hyperparams(model, train_loader, val_loader, optimizer, criterion, epochs, patience, trial):\n",
    "    best_val_accuracy = 0.0\n",
    "    best_val_accuracy_epoch = 0\n",
    "    early_stop_flag = False\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train() # Enable training mode\n",
    "        train_loss = 0.0\n",
    "        total_train_samples = 0\n",
    "        correct_train_predictions = 0\n",
    "\n",
    "        # Wrap train_loader with tqdm for progress visualization\n",
    "        train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs} [Training]\")\n",
    "\n",
    "        for batch in train_loader_tqdm: #Iterates over the train_loader, which is a DataLoader object containing batches of training data.\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad() # Reset gradients\n",
    "            outputs = model(input_ids, attention_mask=attention_mask) # Forward pass\n",
    "            logits = outputs.logits # save the logits (the raw output of the model)\n",
    "            loss = criterion(logits, labels) # Calculate loss\n",
    "\n",
    "            loss.backward() # Backward pass\n",
    "            optimizer.step() # Update weights using the optimizer\n",
    "\n",
    "            # Accumulate training loss and predictions\n",
    "            train_loss += loss.item() * input_ids.size(0)\n",
    "            total_train_samples += input_ids.size(0)\n",
    "            correct_train_predictions += (logits.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "            # Update tqdm description with current loss and accuracy\n",
    "            train_loader_tqdm.set_postfix(loss=loss.item())\n",
    "\n",
    "\n",
    "        train_loss /= total_train_samples\n",
    "        train_accuracy = correct_train_predictions / total_train_samples\n",
    "\n",
    "        ###  Validation loop  ###\n",
    "        model.eval() # Enable evaluation mode\n",
    "        val_loss = 0.0\n",
    "        total_val_samples = 0\n",
    "        correct_val_predictions = 0\n",
    "\n",
    "        all_val_labels = []\n",
    "        all_val_preds = []\n",
    "\n",
    "        # Wrap val_loader with tqdm for progress visualization\n",
    "        val_loader_tqdm = tqdm(val_loader, desc=f\"Epoch {epoch}/{epochs} [Validation]\")\n",
    "\n",
    "        with torch.no_grad(): # Disable gradient computation\n",
    "            for batch in val_loader_tqdm: # iterate on the val_loader's batches\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                loss = criterion(logits, labels)\n",
    "\n",
    "                val_loss += loss.item() * input_ids.size(0)\n",
    "                total_val_samples += input_ids.size(0)\n",
    "                correct_val_predictions += (logits.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "                all_val_labels.extend(labels.cpu().numpy())\n",
    "                all_val_preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "                # Update tqdm description with current loss and accuracy\n",
    "                val_loader_tqdm.set_postfix(loss=loss.item())\n",
    "\n",
    "\n",
    "        # calculate metrics\n",
    "        val_loss /= total_val_samples\n",
    "        val_accuracy = correct_val_predictions / total_val_samples\n",
    "        val_precision = precision_score(all_val_labels, all_val_preds, average='macro')\n",
    "        val_recall = recall_score(all_val_labels, all_val_preds, average='macro')\n",
    "        val_f1 = f1_score(all_val_labels, all_val_preds, average='macro')\n",
    "        \n",
    "        # Check for early stopping\n",
    "        best_val_accuracy, best_val_accuracy_epoch, early_stop_flag = early_stop_check(patience, best_val_accuracy, best_val_accuracy_epoch, val_accuracy, epoch)\n",
    "        \n",
    "        if trial is not None:\n",
    "            trial.report(val_f1, step=epoch)\n",
    "\n",
    "            if trial.should_prune():\n",
    "                print(f\"[Optuna] Trial pruned at epoch {epoch}\")\n",
    "                raise optuna.TrialPruned()\n",
    "            \n",
    "        # Save the best model under the best_model_state parameter\n",
    "        if val_accuracy == best_val_accuracy:\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "        # Log metrics to Weights & Biases - THIS IS WHERE WE TRACK THE RESULTS AND THE PROCESS\n",
    "        wandb.log({ #log == logging of the training process (e.g. results) - will be done each epoch\n",
    "            \"Epoch\": epoch,\n",
    "            \"Train Loss\": train_loss,\n",
    "            \"Train Accuracy\": train_accuracy,\n",
    "            \"Validation Loss\": val_loss,\n",
    "            \"Validation Accuracy\": val_accuracy,\n",
    "            \"Validation Precision\": val_precision,\n",
    "            \"Validation Recall\": val_recall,\n",
    "            \"Validation F1\": val_f1})\n",
    "\n",
    "        if early_stop_flag:  # Checks whether the early stopping condition has been met, as indicated by the early_stop_flag\n",
    "            break# Exits the training loop immediately if the early stopping condition is satisfied\n",
    "\n",
    "    if best_model_state is not None: # Save the best model as a .pt file\n",
    "        torch.save(best_model_state, f\"best_model_trial_{trial.number+1}.pt\")\n",
    "        artifact = wandb.Artifact(\"model\", type=\"model\")\n",
    "        artifact.add_file(f\"best_model_trial_{trial.number+1}.pt\")\n",
    "        wandb.log_artifact(artifact)\n",
    "\n",
    "    return best_val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WW-zXQUurIzn",
   "metadata": {
    "id": "WW-zXQUurIzn"
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    model_name = trial.suggest_categorical(\"model_name\", [\n",
    "    \"cardiffnlp/twitter-roberta-base-sentiment\",\n",
    "    \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
    "])\n",
    "\n",
    "    model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=5, ignore_mismatched_sizes=True).to(device) # Initialize RoBERTa model for sequence classification with 5 labels\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "    # Hyperparameter suggestions\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 5e-5)\n",
    "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-7, 0.2)\n",
    "    patience = trial.suggest_int(\"patience\", 7, 10)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64]) # Batch size can be 16, 32, or 64\n",
    "    train_dataset = Tweets(train_df.sample(frac=0.2, random_state=trial.number), tokenizer) \n",
    "    val_dataset = Tweets(val_df, tokenizer) # Create dataset objects for training, validation, and testing \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # insert into a DataLoader\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False) # insert into a DataLoader\n",
    "\n",
    "    \n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 4)\n",
    "    for param in model.roberta.parameters():    # Freeze layers\n",
    "        param.requires_grad = False\n",
    "    for param in model.roberta.encoder.layer[-num_layers:].parameters():     # unfreeze the last \"num_layers\" of the encoder\n",
    "        param.requires_grad = True\n",
    "    for param in model.classifier.parameters():    #unfreeze the classifier\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Initialize Weights & Biases - the values in the config are the properties of each trial.\n",
    "    wandb.init(project=\"moti-matan-tel-aviv-university\",\n",
    "            config={\n",
    "        \"model_name\": model_name,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"patience\": patience,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"num_layers\": num_layers,\n",
    "        \"architecture\": \"RoBERTa\",\n",
    "        \"dataset\": \"datatattle/covid-19-nlp-text-classification\"},\n",
    "        name=f\"{model_name}+trial_{trial.number+1}\") # The name that will be saved in the W&B platform\n",
    "\n",
    "    # Train the model and get the best validation accuracy\n",
    "    best_val_accuracy = train_model_with_hyperparams(model, train_loader, val_loader, optimizer, criterion, epochs=5, patience=patience, trial=trial)\n",
    "\n",
    "    wandb.finish() # Finish the Weights & Biases run\n",
    "\n",
    "    return best_val_accuracy # Return best validation acc as the objective to maximize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "nnvAY0pyrLZu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "id": "nnvAY0pyrLZu",
    "outputId": "f7facf29-734d-41fb-8794-fe8d10379507"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 17:45:08,822] A new study created in memory with name: no-name-34afd369-b8af-4a0f-9f47-989fcff36c88\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_2515/2159930532.py:10: FutureWarning:\n",
      "\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "\n",
      "/tmp/ipykernel_2515/2159930532.py:11: FutureWarning:\n",
      "\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/matan/DL_env/wandb/run-20250621_174509-xuif9hmx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/xuif9hmx' target=\"_blank\">RobertaForSequenceClassification(<br>  (roberta): RobertaModel(<br>    (embeddings): RobertaEmbeddings(<br>      (word_embeddings): Embedding(50265, 768, padding_idx=1)<br>      (position_embeddings): Embedding(514, 768, padding_idx=1)<br>      (token_type_embeddings): Embedding(1, 768)<br>      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)<br>      (dropout): Dropout(p=0.1, inplace=False)<br>    )<br>    (encoder): RobertaEncoder(<br>      (layer): ModuleList(<br>        (0-11): 12 x RobertaLayer(<br>          (attention): RobertaAttention(<br>            (self): RobertaSdpaSelfAttention(<br>              (query): Linear(in_features=768, out_features=768, bias=True)<br>              (key): Linear(in_features=768, out_features=768, bias=True)<br>              (value): Linear(in_features=768, out_features=768, bias=True)<br>              (dropout): Dropout(p=0.1, inplace=False)<br>            )<br>            (output): RobertaSelfOutput(<br>              (dense): Linear(in_features=768, out_features=768, bias=True)<br>              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)<br>              (dropout): Dropout(p=0.1, inplace=False)<br>            )<br>          )<br>          (intermediate): RobertaIntermediate(<br>            (dense): Linear(in_features=768, out_features=3072, bias=True)<br>            (intermediate_act_fn): GELUActivation()<br>          )<br>          (output): RobertaOutput(<br>            (dense): Linear(in_features=3072, out_features=768, bias=True)<br>            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)<br>            (dropout): Dropout(p=0.1, inplace=False)<br>          )<br>        )<br>      )<br>    )<br>  )<br>  (classifier): RobertaClassificationHead(<br>    (dense): Linear(in_features=768, out_features=768, bias=True)<br>    (dropout): Dropout(p=0.1, inplace=False)<br>    (out_proj): Linear(in_features=768, out_features=5, bias=True)<br>  )<br>)+trial_1</a></strong> to <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/xuif9hmx' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/xuif9hmx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 [Training]:   2%|▏         | 7/412 [00:01<00:52,  7.67it/s, loss=1.54]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 [Training]: 100%|██████████| 412/412 [00:52<00:00,  7.83it/s, loss=1.45] \n",
      "Epoch 1/5 [Validation]: 100%|██████████| 515/515 [00:54<00:00,  9.37it/s, loss=1.16] \n",
      "Epoch 2/5 [Training]: 100%|██████████| 412/412 [00:52<00:00,  7.77it/s, loss=1.5]  \n",
      "Epoch 2/5 [Validation]: 100%|██████████| 515/515 [00:55<00:00,  9.32it/s, loss=1.03] \n",
      "Epoch 3/5 [Training]: 100%|██████████| 412/412 [00:53<00:00,  7.73it/s, loss=1.19] \n",
      "Epoch 3/5 [Validation]: 100%|██████████| 515/515 [00:55<00:00,  9.32it/s, loss=1.04] \n",
      "Epoch 4/5 [Training]: 100%|██████████| 412/412 [00:53<00:00,  7.72it/s, loss=1.04] \n",
      "Epoch 4/5 [Validation]: 100%|██████████| 515/515 [00:55<00:00,  9.32it/s, loss=0.996]\n",
      "Epoch 5/5 [Training]: 100%|██████████| 412/412 [00:53<00:00,  7.76it/s, loss=1.01] \n",
      "Epoch 5/5 [Validation]: 100%|██████████| 515/515 [00:55<00:00,  9.36it/s, loss=1.04] \n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▃▅▆█</td></tr><tr><td>Train Accuracy</td><td>▁▄▆██</td></tr><tr><td>Train Loss</td><td>█▅▃▂▁</td></tr><tr><td>Validation Accuracy</td><td>▁▄▅▆█</td></tr><tr><td>Validation F1</td><td>▁▅▆▇█</td></tr><tr><td>Validation Loss</td><td>█▅▄▃▁</td></tr><tr><td>Validation Precision</td><td>▄▁█▆▇</td></tr><tr><td>Validation Recall</td><td>▁▆▆██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.48352</td></tr><tr><td>Train Loss</td><td>1.1852</td></tr><tr><td>Validation Accuracy</td><td>0.4887</td></tr><tr><td>Validation F1</td><td>0.50448</td></tr><tr><td>Validation Loss</td><td>1.18387</td></tr><tr><td>Validation Precision</td><td>0.51307</td></tr><tr><td>Validation Recall</td><td>0.50589</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">RobertaForSequenceClassification(<br>  (roberta): RobertaModel(<br>    (embeddings): RobertaEmbeddings(<br>      (word_embeddings): Embedding(50265, 768, padding_idx=1)<br>      (position_embeddings): Embedding(514, 768, padding_idx=1)<br>      (token_type_embeddings): Embedding(1, 768)<br>      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)<br>      (dropout): Dropout(p=0.1, inplace=False)<br>    )<br>    (encoder): RobertaEncoder(<br>      (layer): ModuleList(<br>        (0-11): 12 x RobertaLayer(<br>          (attention): RobertaAttention(<br>            (self): RobertaSdpaSelfAttention(<br>              (query): Linear(in_features=768, out_features=768, bias=True)<br>              (key): Linear(in_features=768, out_features=768, bias=True)<br>              (value): Linear(in_features=768, out_features=768, bias=True)<br>              (dropout): Dropout(p=0.1, inplace=False)<br>            )<br>            (output): RobertaSelfOutput(<br>              (dense): Linear(in_features=768, out_features=768, bias=True)<br>              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)<br>              (dropout): Dropout(p=0.1, inplace=False)<br>            )<br>          )<br>          (intermediate): RobertaIntermediate(<br>            (dense): Linear(in_features=768, out_features=3072, bias=True)<br>            (intermediate_act_fn): GELUActivation()<br>          )<br>          (output): RobertaOutput(<br>            (dense): Linear(in_features=3072, out_features=768, bias=True)<br>            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)<br>            (dropout): Dropout(p=0.1, inplace=False)<br>          )<br>        )<br>      )<br>    )<br>  )<br>  (classifier): RobertaClassificationHead(<br>    (dense): Linear(in_features=768, out_features=768, bias=True)<br>    (dropout): Dropout(p=0.1, inplace=False)<br>    (out_proj): Linear(in_features=768, out_features=5, bias=True)<br>  )<br>)+trial_1</strong> at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/xuif9hmx' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university/runs/xuif9hmx</a><br> View project at: <a href='https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university' target=\"_blank\">https://wandb.ai/moti-matan-tel-aviv-university/moti-matan-tel-aviv-university</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250621_174509-xuif9hmx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-21 17:54:13,396] Trial 0 finished with value: 0.48870262390670555 and parameters: {'model_name': 'cardiffnlp/twitter-roberta-base-sentiment', 'learning_rate': 3.7011997967112637e-05, 'weight_decay': 0.007537992022984844, 'patience': 9, 'batch_size': 16, 'num_layers': 1}. Best is trial 0 with value: 0.48870262390670555.\n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base-sentiment and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[W 2025-06-21 17:54:14,602] Trial 1 failed with parameters: {'model_name': 'cardiffnlp/twitter-xlm-roberta-base-sentiment'} because of the following error: ValueError(\"Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/matan/DL_env/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py\", line 1737, in convert_slow_tokenizer\n",
      "    ).converted()\n",
      "      ^^^^^^^^^^^\n",
      "  File \"/home/matan/DL_env/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py\", line 1631, in converted\n",
      "    tokenizer = self.tokenizer()\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/matan/DL_env/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py\", line 1624, in tokenizer\n",
      "    vocab_scores, merges = self.extract_vocab_merges_from_model(self.vocab_file)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/matan/DL_env/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py\", line 1600, in extract_vocab_merges_from_model\n",
      "    bpe_ranks = load_tiktoken_bpe(tiktoken_url)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/matan/DL_env/lib/python3.12/site-packages/tiktoken/load.py\", line 148, in load_tiktoken_bpe\n",
      "    contents = read_file_cached(tiktoken_bpe_file, expected_hash)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/matan/DL_env/lib/python3.12/site-packages/tiktoken/load.py\", line 48, in read_file_cached\n",
      "    cache_key = hashlib.sha1(blobpath.encode()).hexdigest()\n",
      "                             ^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'encode'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/matan/DL_env/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2515/2159930532.py\", line 8, in objective\n",
      "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/matan/DL_env/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py\", line 1032, in from_pretrained\n",
      "    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/matan/DL_env/lib/python3.12/site-packages/transformers/tokenization_utils_base.py\", line 2025, in from_pretrained\n",
      "    return cls._from_pretrained(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/matan/DL_env/lib/python3.12/site-packages/transformers/tokenization_utils_base.py\", line 2278, in _from_pretrained\n",
      "    tokenizer = cls(*init_inputs, **init_kwargs)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/matan/DL_env/lib/python3.12/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py\", line 108, in __init__\n",
      "    super().__init__(\n",
      "  File \"/home/matan/DL_env/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py\", line 139, in __init__\n",
      "    fast_tokenizer = convert_slow_tokenizer(self, from_tiktoken=True)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/matan/DL_env/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py\", line 1739, in convert_slow_tokenizer\n",
      "    raise ValueError(\n",
      "ValueError: Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']\n",
      "[W 2025-06-21 17:54:14,604] Trial 1 failed with value None.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DL_env/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:1737\u001b[39m, in \u001b[36mconvert_slow_tokenizer\u001b[39m\u001b[34m(transformer_tokenizer, from_tiktoken)\u001b[39m\n\u001b[32m   1733\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mConverting from Tiktoken\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTikTokenConverter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1735\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformer_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1736\u001b[39m \u001b[43m        \u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformer_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m1737\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DL_env/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:1631\u001b[39m, in \u001b[36mTikTokenConverter.converted\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1630\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconverted\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Tokenizer:\n\u001b[32m-> \u001b[39m\u001b[32m1631\u001b[39m     tokenizer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1632\u001b[39m     tokenizer.pre_tokenizer = pre_tokenizers.Sequence(\n\u001b[32m   1633\u001b[39m         [\n\u001b[32m   1634\u001b[39m             pre_tokenizers.Split(Regex(\u001b[38;5;28mself\u001b[39m.pattern), behavior=\u001b[33m\"\u001b[39m\u001b[33misolated\u001b[39m\u001b[33m\"\u001b[39m, invert=\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m   1635\u001b[39m             pre_tokenizers.ByteLevel(add_prefix_space=\u001b[38;5;28mself\u001b[39m.add_prefix_space, use_regex=\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m   1636\u001b[39m         ]\n\u001b[32m   1637\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DL_env/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:1624\u001b[39m, in \u001b[36mTikTokenConverter.tokenizer\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1623\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtokenizer\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1624\u001b[39m     vocab_scores, merges = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mextract_vocab_merges_from_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1625\u001b[39m     tokenizer = Tokenizer(BPE(vocab_scores, merges, fuse_unk=\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DL_env/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:1600\u001b[39m, in \u001b[36mTikTokenConverter.extract_vocab_merges_from_model\u001b[39m\u001b[34m(self, tiktoken_url)\u001b[39m\n\u001b[32m   1596\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1597\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`tiktoken` is required to read a `tiktoken` file. Install it with `pip install tiktoken`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1598\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1600\u001b[39m bpe_ranks = \u001b[43mload_tiktoken_bpe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtiktoken_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1601\u001b[39m byte_encoder = bytes_to_unicode()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DL_env/lib/python3.12/site-packages/tiktoken/load.py:148\u001b[39m, in \u001b[36mload_tiktoken_bpe\u001b[39m\u001b[34m(tiktoken_bpe_file, expected_hash)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_tiktoken_bpe\u001b[39m(tiktoken_bpe_file: \u001b[38;5;28mstr\u001b[39m, expected_hash: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[32m    147\u001b[39m     \u001b[38;5;66;03m# NB: do not add caching to this function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m     contents = \u001b[43mread_file_cached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtiktoken_bpe_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_hash\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m     ret = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DL_env/lib/python3.12/site-packages/tiktoken/load.py:48\u001b[39m, in \u001b[36mread_file_cached\u001b[39m\u001b[34m(blobpath, expected_hash)\u001b[39m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m read_file(blobpath)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m cache_key = hashlib.sha1(\u001b[43mblobpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m()).hexdigest()\n\u001b[32m     50\u001b[39m cache_path = os.path.join(cache_dir, cache_key)\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'encode'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[95]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Optuna Study\u001b[39;00m\n\u001b[32m      2\u001b[39m study = optuna.create_study(direction=\u001b[33m\"\u001b[39m\u001b[33mmaximize\u001b[39m\u001b[33m\"\u001b[39m,pruner=MedianPruner(n_warmup_steps=\u001b[32m1\u001b[39m))  \u001b[38;5;66;03m# Specifies that the goal of the optimization is to maximize the objective function\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DL_env/lib/python3.12/site-packages/optuna/study/study.py:489\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    388\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    389\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    397\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    398\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    399\u001b[39m \n\u001b[32m    400\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    487\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    488\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DL_env/lib/python3.12/site-packages/optuna/study/_optimize.py:64\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     77\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DL_env/lib/python3.12/site-packages/optuna/study/_optimize.py:161\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     frozen_trial = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DL_env/lib/python3.12/site-packages/optuna/study/_optimize.py:253\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    246\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    249\u001b[39m     frozen_trial.state == TrialState.FAIL\n\u001b[32m    250\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    251\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    252\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DL_env/lib/python3.12/site-packages/optuna/study/_optimize.py:201\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    203\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    204\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[94]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mobjective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m      2\u001b[39m     model_name = trial.suggest_categorical(\u001b[33m\"\u001b[39m\u001b[33mmodel_name\u001b[39m\u001b[33m\"\u001b[39m, [\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcardiffnlp/twitter-roberta-base-sentiment\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcardiffnlp/twitter-xlm-roberta-base-sentiment\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m ])\n\u001b[32m      7\u001b[39m     model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=\u001b[32m5\u001b[39m, ignore_mismatched_sizes=\u001b[38;5;28;01mTrue\u001b[39;00m).to(device) \u001b[38;5;66;03m# Initialize RoBERTa model for sequence classification with 5 labels\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fast\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# Hyperparameter suggestions\u001b[39;00m\n\u001b[32m     10\u001b[39m     learning_rate = trial.suggest_loguniform(\u001b[33m\"\u001b[39m\u001b[33mlearning_rate\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1e-5\u001b[39m, \u001b[32m5e-5\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DL_env/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:1032\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1029\u001b[39m tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[32m   1031\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m1032\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class_fast\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1033\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1034\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DL_env/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2025\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2022\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2023\u001b[39m         logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2025\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2026\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2027\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2028\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2029\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2030\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2031\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2032\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2033\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2034\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2035\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2036\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2037\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DL_env/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2278\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2276\u001b[39m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[32m   2277\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2278\u001b[39m     tokenizer = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2279\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n\u001b[32m   2280\u001b[39m     logger.info(\n\u001b[32m   2281\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2282\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2283\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DL_env/lib/python3.12/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py:108\u001b[39m, in \u001b[36mXLMRobertaTokenizerFast.__init__\u001b[39m\u001b[34m(self, vocab_file, tokenizer_file, bos_token, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, **kwargs)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     93\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     94\u001b[39m     vocab_file=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    104\u001b[39m ):\n\u001b[32m    105\u001b[39m     \u001b[38;5;66;03m# Mask token behave like a normal word, i.e. include the space before it\u001b[39;00m\n\u001b[32m    106\u001b[39m     mask_token = AddedToken(mask_token, lstrip=\u001b[38;5;28;01mTrue\u001b[39;00m, rstrip=\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mask_token, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m mask_token\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbos_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m        \u001b[49m\u001b[43meos_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m        \u001b[49m\u001b[43msep_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43msep_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcls_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcls_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m        \u001b[49m\u001b[43munk_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmask_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m     \u001b[38;5;28mself\u001b[39m.vocab_file = vocab_file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DL_env/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:139\u001b[39m, in \u001b[36mPreTrainedTokenizerFast.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m     \u001b[38;5;28mself\u001b[39m.vocab_file = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mvocab_file\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    138\u001b[39m     \u001b[38;5;28mself\u001b[39m.additional_special_tokens = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33madditional_special_tokens\u001b[39m\u001b[33m\"\u001b[39m, [])\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     fast_tokenizer = \u001b[43mconvert_slow_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tiktoken\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m     slow_tokenizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DL_env/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:1739\u001b[39m, in \u001b[36mconvert_slow_tokenizer\u001b[39m\u001b[34m(transformer_tokenizer, from_tiktoken)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m TikTokenConverter(\n\u001b[32m   1735\u001b[39m         vocab_file=transformer_tokenizer.vocab_file,\n\u001b[32m   1736\u001b[39m         additional_special_tokens=transformer_tokenizer.additional_special_tokens,\n\u001b[32m   1737\u001b[39m     ).converted()\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1740\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConverting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1741\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mwith a SentencePiece tokenizer.model file.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1742\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCurrently available slow->fast converters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(SLOW_TO_FAST_CONVERTERS.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1743\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']"
     ]
    }
   ],
   "source": [
    "# Optuna Study\n",
    "study = optuna.create_study(direction=\"maximize\",pruner=MedianPruner(n_warmup_steps=1))  # Specifies that the goal of the optimization is to maximize the objective function\n",
    "study.optimize(objective, n_trials=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed130366",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_weights.pt\")\n",
    "artifact = wandb.Artifact(\"model\", type=\"model\")\n",
    "artifact.add_file(\"model_weights.pt\")\n",
    "wandb.log_artifact(artifact)\n",
    "# Save the study results\n",
    "torch.save(model.state_dict(), \"model2_weights.pt\")\n",
    "artifact = wandb.Artifact(\"model2\", type=\"model\")\n",
    "artifact.add_file(\"model2_weights.pt\")\n",
    "wandb.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d12591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ניקוי זיכרון מה-GPU\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# ניקוי זיכרון מה-Python\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8mT8__SBrOog",
   "metadata": {
    "id": "8mT8__SBrOog"
   },
   "outputs": [],
   "source": [
    "# Function to evaluate the model\n",
    "def evaluate_model(model_path, test_loader):\n",
    "    # Load the model\n",
    "    model.load_state_dict(torch.load(model_path)) # loading the trained model\n",
    "    model = model.to(device)\n",
    "    model.eval() # eval mode\n",
    "\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    # Wrap test_loader with tqdm for progress visualization\n",
    "    test_loader_tqdm = tqdm(test_loader, desc=\"Evaluating Model\")\n",
    "\n",
    "    #same idea... just testing and getting resutls...\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader_tqdm:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = logits.argmax(dim=1)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "            # Update tqdm description (optional, can show batch progress)\n",
    "            test_loader_tqdm.set_postfix(batch=test_loader_tqdm.n)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='binary')\n",
    "    recall = recall_score(all_labels, all_preds, average='binary')\n",
    "    f1 = f1_score(all_labels, all_preds, average='binary')\n",
    "\n",
    "    return {\"Accuracy\": accuracy, \"Precision\": precision, \"Recall\": recall, \"F1 Score\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "S8t6WVAqrRKt",
   "metadata": {
    "id": "S8t6WVAqrRKt"
   },
   "outputs": [],
   "source": [
    "# Load the test data set\n",
    "test_dataset = Tweets(test_df, tokenizer.from_pretrained('roberta-large', use_fast=False)) # Create a dataset object for the test set\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZWKQGuhZrUNg",
   "metadata": {
    "id": "ZWKQGuhZrUNg"
   },
   "outputs": [],
   "source": [
    "# Test multiple models\n",
    "model1_path= 'cardiffnlp/twitter-roberta-base-sentiment'\n",
    "model2_path = 'cardiffnlp/twitter-xlm-roberta-base-sentiment'\n",
    "\n",
    "model_paths = [model1_path, model2_path]  # Replace with actual model paths\n",
    "for model_path in model_paths:\n",
    "    metrics = evaluate_model(model_path, test_loader)\n",
    "    print(f\"Metrics for {model_path}:\")\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44ba882",
   "metadata": {},
   "source": [
    "### FT using HF trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f93df2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b623a892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023503fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d1187a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0833dbe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "347bd4b8",
   "metadata": {},
   "source": [
    "### Model Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bbec3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "from torch.nn import functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19fd65e",
   "metadata": {},
   "source": [
    "#### Metrics definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98282436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def size_of_model(model) -> int:\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def non_zero_parameters_count(model):\n",
    "    return sum((p != 0).sum().item() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68a0dcb",
   "metadata": {},
   "source": [
    "#### Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93298953",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.quantization import quantize_dynamic\n",
    "\n",
    "def compress_quantize_model(model):\n",
    "    return quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a7348a",
   "metadata": {},
   "source": [
    "#### Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403d4488",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import prune\n",
    "\n",
    "def compress_prune_model(model, prune_percent = 0.4):\n",
    "    model_to_prune = model\n",
    "    parameters_to_prune = [ (module, 'weight') for module in model_to_prune.modules() if isinstance(module, nn.Linear)]\n",
    "    prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=prune_percent)\n",
    "    return model_to_prune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c11035",
   "metadata": {},
   "source": [
    "#### Knowledge Distilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da12fd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillationTrainer(Trainer):\n",
    "    def __init__(self,  teacher_model=None, temperature=2.0, alpha=0.5, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher = teacher_model\n",
    "        self.temperature = temperature\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        outputs_student = model(**inputs)\n",
    "        with torch.no_grad():\n",
    "            outputs_teacher = self.teacher(**inputs)\n",
    "\n",
    "        loss_ce = F.cross_entropy(outputs_student.logits, inputs[\"labels\"])\n",
    "        loss_kl = F.kl_div(\n",
    "            F.log_softmax(outputs_student.logits / self.temperature, dim=-1),\n",
    "            F.softmax(outputs_teacher.logits / self.temperature, dim=-1),\n",
    "            reduction=\"batchmean\") * (self.temperature ** 2)\n",
    "        loss = self.alpha * loss_ce + (1 - self.alpha) * loss_kl\n",
    "        \n",
    "        return (loss, outputs_student) if return_outputs else loss\n",
    "\n",
    "\n",
    "def compress_distilled_model(model):\n",
    "    student_distilled_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                            'distilroberta-base',\n",
    "                             num_labels=5)\n",
    "    trainer_distill = DistillationTrainer(\n",
    "    model=student_distilled_model,\n",
    "    teacher_model=model,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"test\"],\n",
    "    compute_metrics=compute_metrics)\n",
    "\n",
    "    trainer_distill.train()\n",
    "    return student_distilled_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23cff43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef1078e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.quantization\n",
    "import time\n",
    "import pandas as pd\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def evaluate_model_to_compressed(model, test_dataloader, device='cpu'):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    inference_times = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            start_time = time.time()\n",
    "            outputs = model(inputs)\n",
    "            inference_times.append(time.time() - start_time)\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    avg_inference_time = 1000 * sum(inference_times) / len(inference_times)  # ms per batch\n",
    "\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    nonzero_params = sum(torch.count_nonzero(p).item() for p in model.parameters())\n",
    "    size_mb = sum(p.element_size() * p.numel() for p in model.parameters()) / 1e6\n",
    "\n",
    "    return {\n",
    "        'accuracy (%)': accuracy,\n",
    "        'inference_time (ms)': avg_inference_time,\n",
    "        'total_params': num_params,\n",
    "        'nonzero_params': nonzero_params,\n",
    "        'model_size (MB)': size_mb,\n",
    "        'sparsity (%)': 100 * (1 - nonzero_params / num_params)\n",
    "    }\n",
    "# Pruning\n",
    "def apply_pruning(model):\n",
    "    model = copy.deepcopy(model)\n",
    "    parameters_to_prune = [(module, 'weight') for module in model.modules() if isinstance(module, nn.Linear)]\n",
    "    prune.global_unstructured(parameters_to_prune, pruning_method=prune.L1Unstructured, amount=0.5)\n",
    "    return model\n",
    "\n",
    "# Quantization (static)\n",
    "def apply_quantization(model):\n",
    "    model = copy.deepcopy(model)\n",
    "    model.eval()\n",
    "    model.qconfig = torch.quantization.default_qconfig\n",
    "    torch.quantization.prepare(model, inplace=True)\n",
    "    torch.quantization.convert(model, inplace=True)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def collect_compression_metrics(model):\n",
    "    results = {}\n",
    "    results['original'] = evaluate_model_to_compressed(model)\n",
    "    pruned_model = apply_pruning(model)\n",
    "    results['pruned'] = evaluate_model_to_compressed(pruned_model)\n",
    "    try:\n",
    "        quantized_model = apply_quantization(model)\n",
    "        results['quantized'] = evaluate_model_to_compressed(quantized_model)\n",
    "    except Exception:\n",
    "        results['quantized'] = {k: np.nan for k in results['original'].keys()}  # Fallback if quant fails\n",
    "\n",
    "    # Distilled (simulated)\n",
    "    distilled_model = SmallModel()\n",
    "    results['distilled'] = evaluate_model_to_compressed(distilled_model)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(results).T\n",
    "\n",
    "    # Percent difference from best per metric\n",
    "    percent_diff = df.apply(lambda col: 100 * (col - col.min()) / col.min() if col.dtype != 'O' else col)\n",
    "    percent_diff.columns = [f\"{col} Δ%\" for col in df.columns]\n",
    "    df = pd.concat([df, percent_diff], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Create and evaluate\n",
    "model = SimpleModel()\n",
    "compression_metrics_df = collect_compression_metrics(model)\n",
    "compression_metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02257e8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fc0d43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eda06c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0550aeb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
